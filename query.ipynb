{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating search client with - brandspeak-index\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient \n",
    "from azure.search.documents.models import (\n",
    "    VectorizedQuery,\n",
    "    VectorFilterMode,    \n",
    ")\n",
    "from backend.tools.searchtool import Search\n",
    "  \n",
    "# Configure environment variables  \n",
    "load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "credential = AzureKeyCredential(key)\n",
    "openai_client = OpenAI()\n",
    "openai_model = os.getenv(\"OPENAI_MODEL\")\n",
    "\n",
    "search_client: Search = Search('sales_vector_index')  # get instance of search to query corpus using the name of the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model='text-embedding-ada-002'):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai_client.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Search - satya keynote nvidia\n",
      "Got embedding\n",
      "Got result\n",
      "[SOURCEFILE:  Full Keynote Satya Nadella at Microsoft Ignite 2023.txt]One year ago, at the dawn of a new age, we took the first bold step into a world of unlimited possibilities. We're going to get to think about how we use our imagination to solve some of the biggest problems in the world. It allows for the patient and doctor to be a patient or a doctor like it used to be. If AI as your co-pilot, you are supercharging innovation. It's like more than my mind. It's magic. You built new solutions and inspired us to imagine more. In our journey of Generative AI, we have over 700 use cases across all divisions and functions, so we really see a big impact. Co-pilot makes day-to-day life easier so you can spend your time thinking how I want to improve the experience for people learning a language. This technology has the potential to completely reimagine the way every single student learns in the world. It enables all people to get back to what they're brilliant at. You redefined the art of collaboration. I've been using co-pilot and on exciting problems that are having real impacts in people's lives. I can just go home and be like, yeah, I did that. I'm a superhuman. I've done something amazing. This is the age of co-pilot. Good morning. Good morning. Good morning and welcome to Ignite. It's great to be together in person right here in Seattle and all of you joining from all over the world. Welcome. We know when we schedule Ignite that we will schedule it on the same day that there's a World Cup semi-final going on in cricket. I've been up all night, but it finished five minutes ago. I'm glad I did. This is the short version of the game, by the way. Here we are. It's just been a fantastic last 12 months. It's hard to believe that it's just been a year since Chattcheapy T first came out. Lots been done. Lots going on in terms of the pace of innovation, which has just been astounding. Just last week I was at the OpenAI Dev Day and GitHub Universe and now of course Ignite. The interesting thing is we're entering this exciting new face of AI, where we're not just talking about it as technology that's new and interesting, but we're getting into the details of product making, deployment, safety, real productivity gains, all the real world issues and that's just the most exciting thing for all of us as builders. We're at a tipping point. This is clearly the age of co-pilots from digital natives like Airbnb or Dio Lingo to shopify as well as the world's largest companies, whether it's BT or Bear or Densu, Goodya, Lumen, all are deploying the Microsoft co-pilot to companies in every industry who are building their own co-pilots, from L-second finance to Epic in healthcare to rock well automation in manufacturing and Siemens in manufacturing. It's fantastic to see people deploy their own co-pilots. And today we're sharing lots of new data that shows real productivity gains that co-pilot is already driving. We're taking a very broad lens to deeply understand the impact of co-pilot on both the creativity and the productivity. And the results are pretty striking. With co-pilots, we're able to complete tasks much faster. And that's having a real cascading effect on work and workflow everywhere. People who use co-pilot are spending less time searching for information. They're holding more effective meetings. They're able to collaborate on work artifacts, whether they're word documents, spreadsheets, emails, all of them have richer context about their role, about their organization so that they can collaborate much more and stay in focus. And of course, we're just getting started. The way to think about this is co-pilot will be the new UI that helps us gain access to the world's knowledge and your organization's knowledge. But most importantly, it's your agent that helps you act on that knowledge. So this week at Ignite, we're introducing a hundred new updates across every layer of the stack to help us realize that vision. Our end-to-end co-pilot stack for every organization spans the infrastructure, the foundation models, data, tool chains, and of course the co-pilot itself. And today I'll highlight five key themes across everything we're showing you this week. So let's dive right in. It starts with AI infrastructure layer and our approach to Azure as the world's computer. We are offer the most comprehensive global infrastructure with more than 60 data center regions, more than any other provider. Being the world's computer means that we need to be even the world's best systems company across heterogeneous infrastructure. We work closely with our partners across the industry to incorporate the best innovation from power to the data center to the rack to the network to the core compute, as well as the AI accelerators. And in this new age of AI, we are redefining everything across the fleet and the data center. So let's start on how we power the data center. As we build them, we are working to source renewable power. In fact, today we are one of the largest buyers of renewable energy around the globe. We have sourced over 19 gigawatts of renewable energy since 2013. I mean, just to put that in perspective, that's the equivalent to the annual production of 10 Hoover dams. And we're working with producers to bring new energy from wind, solar, geothermal, and nuclear fusion as well. And as we pursue this ambition to not just be carbon free, but to even erase our historical carbon emissions, I'm really excited to share that we are on track to meet our target of generating 100% of the energy we use in our data centers from zero carbon sources by 2025. Now, let's talk about the network that connects our data centers. It's one of the most advanced and extensive in the world already. And to meet the demands of AI and the future workloads, we're driving up the speeds. Our breakthrough hollow core fiber technology is delivering a 47% improvement in speed because photons are able to travel through these microscopic air capillaries instead of through solid glass fiber. This is really cutting edge technology. In fact, we are manufacturing this fiber ourselves in the world's only dedicated factory for hollow core fiber production. Our first deployment, in fact, is already live connecting our data centers in the United Kingdom. We are very, very excited about this. And now, let's step right into the data center. Today, I'm excited to announce the general availability of Azure Boost. You know, it's fantastic to see this new system that offloads server virtualization processes onto purpose build software and hardware. This enables massive improvements in networking, remote storage and local storage throughput, making Azure the best cloud for high performance workloads while strengthening security as well. Now let's go inside our servers. We're tapping into the innovation across the industry, including from our partners AMD and Intel and making that available to you. For example, organizations like Vestus use AMD on high-end compute and memory optimized service in Azure to run simulations on massive amounts of weather data. And the largest SAP database deployments are powered by our new MBB3 virtual machines running the fourth generation of Intel's Zian scalable processors supporting up to 32 terabytes of memory. In fact, Intel's putting their own SAP instances on these machines and it's great to see. You know, as a hyperscaler, we see workloads, we learn from them and then get this opportunity as a systems company to optimize the entirety of the stack. From the energy draw to the silicon to maximizing performance and efficiency. And it's really thanks to this feedback cycle that I'm thrilled to introduce our very first custom in-house CPU series Azure Cobalt starting with Cobalt 100. Cobalt is the first CPU designed by us specifically for the Microsoft Cloud. And this 64-bit 128 core ARM base chip is the fastest of any cloud provider. And it's already powering parts of Microsoft Teams, Azure communication services, as well as Azure SQL as we speak. And next year, we will make this available to customers. Yeah. And when it comes to AI accelerators, we're also partnering broadly across the industry. To make Azure the best cloud, no questions asked for both training and inference. It starts with our very deep partnership with Nvidia. We have built the most powerful AI supercomputing infrastructure in the cloud using Nvidia GPUs. And OpenAI has used this infrastructure to deliver the leading LLMs as we speak. In fact, last week, Azure was the largest submission to ML Perth benchmarking consortium with 10,000 H100 GPUs, three times more than the previous record delivering better performance than any other cloud. And in the latest top 500 list of world supercomputers, Azure was the most powerful supercomputer in the public cloud and third all up. That made news. What didn't make news is we didn't submit the entirety of our super computer, which submitted only a fraction of our super computers. I'm thrilled to be number three with that. As we, and by the way, that's the only one that made the list as a public cloud. And as we build super computers to train these leading large models, InfiniBand gives us a unique advantage. And today we're even going further. We're adding Nvidia's latest GPU AI accelerator, H200, to our fleet to support even larger model infencing with the same latency, which is so important as these models become much bigger and more powerful. The ability for us to have this new generation of accelerators is a big deal. We are also introducing the first preview of Azure Confidential GPU VMs. As you can run your AI models on sensitive data sets on our cloud. We co-design this with Nvidia. So if you're doing what is referred to as retrieval augmented generation or rag, you'll hear a lot about this throughout the conference. Running on this confidential GPU VM, you can enrich, for example, your prompt with very query-specific knowledge from proprietary databases, document archives, while keeping the entirety of the process protected end to end. And so it's very exciting to see us not just need with GPUs, but need with GPUs with confidential computing. Now let's talk about AMD. I'm excited to announce that AMD's flagship MI 300X AI accelerator is coming to Azure to give us even more choice for AI-optimized VMs. With 192 gigabytes of high bandwidth memory and 5.2 terabytes per second of bandwidth, the MI 300X offers industry-leading memory speed and capacity. Again, this means we can serve large models faster using fewer GPUs. We've already got GPT-4 up and running on MI 300X, and today we're offering early access to select customers. And we're not stopping there. We are committed to taking the entirety of our know-how from across the system and bringing you the best innovation from our partners and us. Today we're announcing our first fully custom in-house AI accelerator, Azure Maya. Starting with my 100 designed to run in cloud AI workloads like LLM training and inference, this chip is manufactured on a 5 nanometer process, has 105 billion transistors, making it one of the largest chips that can be made with current technology. And it goes beyond the chip though. We have designed Maya 100 as an end-to-end rack for AI, as you can see right here. AI power demands required infrastructure that is dramatically different from other clouds. The compute workloads require a lot more cooling as well as in their networking density. And we have designed the cooling unit known as the sidekick to match the thermal profile of the chip and added rack-level close-loop liquid cooling for higher efficiency. This architecture allows us to take this rack and put it into existing data center infrastructure and facilities rather than building new ones. And by the way, they're also built and manufactured to meet our zero-waste commitments. So we are very very excited about Maya with Maya. We are combining the state-of-the-art silicon packaging techniques, ultra-high bandwidth networking design, modern cooling power management, algorithmic co-design of both the hardware and the software. And we're already testing this with many of our own AI services including the GitHub Co-Pilot. And we will roll out Maya Accelerator's across-off lead supporting our own workloads first and will scale it to third-party workloads after. This silicon diversity is what allows us to power the world's most powerful foundation models. And all of our AI workloads from Co-Pilot to your own AI applications so that when I say systems, this is the end-to-end innovation. From glass blowing, the next generation fiber optic cables, sourcing renewable energy, designing new approaches to thermal distribution, innovating in silicon, our goal is to ensure that the ultimate efficiency performance and scale is something that we can bring to you from us and our partners. Now let's go to the next layer of the stack, the foundation models. These are only possible of course because of these advanced systems I talked about. Generated AI models span from trillions of parameter LLMs that require these most powerful GPUs in Azure to a few billion parameter task-specific small-language models or SLMs. And we offer the best selection of frontier models which you can use to build your own AI apps while meeting your specific cost, latency, and performance needs. And it starts with our deep, deep partnership with OpenAI. They're just doing stunning breakthrough work to advance the state-of-AI models. And we are thrilled to be all in on this partnership together. And our promise to you is simple. As OpenAI innovates, we will deliver all of that innovation as part of Azure AI. And we are bringing very latest of GPT-4, including GPT-4 Turbo, GPT-4 Turbo with Vision to Azure OpenAI service. Yeah, we can clap. You know, GPT-4 Turbo offers lower pricing, structured JSON formatting, which is sort of my favorite, an extended prompt length. You can in fact fit 300 pages of text, and now into a single prompt. GPT-4 Turbo will be available in Azure OpenAI service this week in preview. And the token pricing for the new models will be at parity with OpenAI. Also soon, you'll be able to connect GPT-4 Turbo with Vision to Azure AI Vision, allowing you to prompt with video images and text. In fact, our customer WPP is already using this today with one of their largest clients. Take a look at the video behind me. Pretty amazing to see video prompts as inputs and with summaries coming out on the other end. It's fantastic to see it. Finally, we will be introducing fine tuning of GPT-4 and Azure OpenAI service as well, allowing you to bring your own data to create these custom versions of GPT-4. You know, we are also all in on open source. And we want to bring the best selection of open source models to Azure and do so responsibly. Our model catalog has the broadest selection of models already, and we are adding even more to our catalog. With stable diffusion, you can jump. generate beautiful immersive images. With CodeLama, you can generate code, Mistral 7B, you can translate and summarize text with NVIDIA's Neematron three family of models. You can build general purpose AI apps. All these capabilities are deeply, deeply integrated with our safety guardrails. And today, we are taking one more big step in support of open source models. We are adding a new models as a service offering in Azure. You know, this, yeah, it is a big deal. It makes it simple. Because this will allow you to get access to these large models that are all available in open source. As just hosted APIs, right, without you as developers having to provision GPUs so that you can focus on development, not back end operations. We are excited to be partnering with Meta. On this, it starts with Lama 2 as a service. You can fine tune Lama 2 with your data to help the model understand your domain better and generate more accurate predictions. We want to support models in every language and in every country. And we are partnering with Mistral to bring their premium models as a service, as well as with Group 42 to bring Jaze, the world's highest quality Arabic language model, again, just as a service. Now, when we talk about open source, there's one more very exciting thing that's happening in this space. And that is SLMs. Microsoft loves SLMs. In fact, one of the best is Fy. A model that is built by Microsoft Research on highly specialized data sets, which can rival models that are even 50 times bigger. In fact, 5.5 has only 1.3 billion parameters, but Nandalens, it just demonstrates state-of-the-art performance against benchmarks testing, things like common sense, language understanding and logical reasoning. And today, I am thrilled to announce 5.2. You know, it's a scaled-up version of 5.5 that shows even better capabilities across all of these benchmarks, while staying pretty small, I mean, relatively small, at 2.7 billion parameters. In fact, it's 50% better at mathematical reasoning, and 5.2 is open source and will be coming to our catalog as well as models of service. Once you have these models, you know, the next up is the tooling consideration. With Azure AI Studio, we offer the full lifecycle tool chain for you to be able to build, customize, train, evaluate and deploy the latest next generation models. It also includes built-in safety tooling. Safety is the most important feature of our AI platform. It's not something we bolt on later, but we are shifting left from the very beginning. And with Azure AI Studio, you can detect and filter harmful user-generated and AI-generated content in your applications as well as your service. The other thing we're doing with Azure AI Studio is extending it to any endpoint, starting with Windows. You can customize state-of-the-art SLMs and leverage our templates for common developer scenarios. So that you can integrate these models right into your applications. When you combine the power of the cloud and the edge, it unlocks super compelling scenarios. Let's say you want to build an NPC helper for a game. You can start with an SLM like FI or as your target model in Windows. We then help you compose solutions to steer your game to do what it needs, like the retrieval augmented generation templates to apply on your data set, to answer questions about quests. This can all happen locally on your Windows machine. The NPC can guide players with their quests or even generate complete new storylines based on prompts from players. For more advanced use cases, you can adapt and fine-tune the SLM on Azure, specifically for your game, using the power of even frontier models like GPD4. It's incredibly powerful to see all this come together. And of course, we're not stopping there. Earlier I mentioned our partnership with Nvidia. Together, we're innovating to make Azure the best cloud for training and inference. Our collaboration extends across the entirety of the stack, including our best-in-class solutions for AI development. Today, we are expanding our partnership by bringing Nvidia's Generative AI Foundry Service to Azure. Applause Really brings together Nvidia's foundation models, frameworks, tools, as well as its DGX Cloud AI, supercomputing and services to provide the best end-to-end solution for creating generative AI models and custom-generative models. To share more, I would like to invite up on stage the Nvidia founder, President CEO, Jensen Wong, to join me. Applause Jensen, thank you so much for being here. This partnership, I talked a lot about all the things we have been doing on the system side. Of course, we wouldn't have been able to train the Open AI models or make all this progress over the last few years without the unbelievable systems we've done. But today, we're even going a step beyond, bringing in fact all of the software innovation that you're doing. Do you want to share a little bit about what we're doing or are you doing on the software side on Azure? I would love to. First of all, I'm so happy to be here to celebrate the amazing work of our two teams. This last 12 months, when I was just listening to unbelievable amount of progress for the whole computer industry, frankly, in the last 12 months. Well, our two teams have been super busy. AI and accelerated computing is a full stack challenge. And it's a data center scale challenge from computing to networking from chips to APIs. Everything has been transformed as a result of generative AI. Now, over the last 12 months, we've been our two teams have been accelerating everything we could. Now, one of the initiatives, of course, is accelerated computing, offloading general purpose computing, accelerating all the software we can because it improves energy efficiency, reduces carbon footprint, reduces cost for our customers, improves their performance, so on and so forth. We built the world's fastest AI supercomputer together. Yeah. It usually takes a couple of two, three years to plan one, easily a year to stand one up. Our two teams built two of them, twins, one in your house, one in my house. We did it, and we stood it up in just a few months. It is now the fastest AI supercomputer world. Applause. Applause. And it's seemingly without barely even trying, it's the third fastest supercomputer on the planet. It's really quite amazing. We worked on all kinds of computer breakthroughs, computing breakthroughs, confidential computing, of course, a very big deal in invention between our two companies, all the way to deploying large language models from the cloud to the PC. The work that we did together so that windows can now be a first class client for large language models, opens up a few hundred Nvidia power PCs and workstations around the world. Yeah, it's a lot of just install base on the edge of very powerful AI machines. It happens to be Windows PCs with GPUs from that day. That's right. That's right. And now with Studio AI, unbelievable. Everybody could be a Ragnar developer. Everybody could engage large language models. Now, we've also, and this is something that I'm so proud of, we talked about a year and a half ago. And this is such a great idea, such a great vision. And you really deserve so much credit for transforming Microsoft's entire culture to be so much more collaborative and partner oriented that Nvidia's platforms, and you invited Nvidia's ecosystem, all of our software stacks to be hosted on Azure. Today, we're announcing the two largest software stacks of our company and Nvidia omniverse. And in fact, just now you saw the WPP video. In fact, that's actually computer graphics. That computer graphics is running on omniverse. And now you can connect omniverse to generative AI. The second, and so omniverse is for industrial digitalization. Today, we're announcing that omniverse cloud, omniverse, which is a stack originally on prem on large computers now available on Azure Cloud. The second is a brand new thing that we're announcing. And you just mentioned that we're offering an AI Foundry Service. Generative AI has opened up the opportunity for every enterprise in the world to engage artificial intelligence. For the very first time, it is now useful, versatile, quite frankly easy to use. And companies all over the world will use it in multiple ways, but here's three basic ways. One, of course, public cloud services like ChatGPT. Second, embed it into applications like Windows. We are very, very happily, also a full-site license customer of Copilot. And so we are going to be augmented by Microsoft Copilets. And if you think that Nvidia is moving fast now, we are going to be turbocharged by Copilot. And then third, of course, customers want to build their own AI's. Their own, they want to create their own, using their own data, create their own proprietary large language models, and create their own rigs. And so today leveraging what Nvidia core assets are, our AI expertise, our AI end-to-end workflow and video AI enterprise, and our AI factories, which is now available on Azure called DJX Cloud. We are going to make these built on these three pillars help customers build their own custom large language models. We are going to do for people who want to build their own proprietary large language models. What TSMC does for us. Yeah, it's fantastic. And so we'll be a foundry for AI models. It's just so amazing to see. I mean, us partnering on everything on the system side and everything up the stack on the software side, whether it's on on New Wars or DJX Cloud and this AI foundry is fantastic. I love that metaphor of TSMC for AI model development. You know, talking about this arc of AI gents, and of course you're being at the core of this long before it became fashionable to talk about it, you were talking about it. What's your arc here of AI innovation going forward? Well, Genevieve to the AI is the single most significant platform transition in computing history. You and I both been in the computer industry a long time. In the last 40 years, nothing has been this big. It's bigger than PC, it's bigger than mobile, it's going to be bigger than internet, and surely by far. This is also the largest TAM expansion of the computer industry in history. There's a whole new type of data center that's now available. Unlike the data centers of the past, this data center is dedicated to one job and one job only, running AI models and generating intelligence. It's an AI factory. This AI factory, you're building, you know, some of the world's most advanced, you're building the world's computer, that computer is now going to be augmented by AI factories all over. The second TAM expansion is where our industry has focused on building tools at the past, now you have co-pilots that use the tools. So in hardware, there's a brand new segment, AI factories. In software, there's a brand new segment, co-pilots. These are brand new things that the world's never had the opportunity to enjoy. Big huge TAM expansion. The first wave is the wave that we enjoyed, which is incredible startups at OpenAI and others who are part of the generous of AI startups, Cloud Internet services. That's the first wave. We're now beginning the second wave and is really triggered and kicked off by co-pilot, office or Windows 365 co-pilot, basically the enterprise generation. The third generation, the third wave, is the wave that I think is going to be the largest wave of all. And the reason for that is because the vast majority of the world's industries run on it, which is heavy industries. And this is where NVIDIA's omniverse and generative AI is going to come together to help heavy industries digitalize and benefit from generative AI. So we're really, quite frankly, barely in the middle of the first wave, starting the second wave. This is going to be... I love that three waves and all happening somewhat in parallel, but they're staging of it. And I think it all upcrues. It compounds across all three. Maybe we can close out, Jensen, you and I work together for decades, Microsoft and NVIDIA work together for decades. You know, partnerships are these magical things where your innovation, our innovation comes together ultimately to enable people in the audience. So just talk about like, when you think about the Microsoft partnership, what's your vision for it, what's your expectations of it, and just any thoughts on that? Well, we have a giant partnership. And many of you are partners with Microsoft here. And I think you all agree with me that there's just a profound transformation in the way that Microsoft works with the ecosystem in the industry. We are suppliers to you, building the most advanced computers together, your suppliers to us. And so we're customer partners with each other. But one of the things that I really, really love is the fact that we partner on advancing fundamental computer science like confidential computing and generative AI and all the infrastructure that we built together. I love that we're inventing new technologies together. But I really love that you're hosting our native stack, right there in Azure. And as a result, we're ecosystem partners. NVIDIA has a rich ecosystem of developers, all of the world. Several million coulda developers, some 15,000 startups around the world works on NVIDIA's platform. The fact that they could now take their stack and without modification run it perfectly on Azure. My developers become your customers. My developers also have the benefit of integrating with all of the Azure APIs and services, and the secure storage, the confidential computing. And so all of that richness amplifies NVIDIA's ecosystem. And so I think this partnership is really quite unique. I think that there's not one like it. We don't have one like it. We're incredibly proud of the partnership and incredibly proud of the work that we do together. And so I. Thank you so much, Jensen. I really deeply appreciate everything that you and your team have been doing. As you said, the last 12 months have been, unlike anything I've seen in my professional career. And we're obviously setting pace and we plan to continue to do so. Thank you so much for your deep partnership. Thank you. Jensen won. All right. So let's go one more layer up the stack to data. You know, it's perhaps one of the most important considerations. Because in some sense, there is no AI without data. Microsoft Fabric brings all your data as well as your analytic workloads into this one ununified experience. Fabric has been our biggest data launch, perhaps in SQL Server. And the reception to the preview has been just incredible. 25,000 customers have already using it. And today, I am thrilled to announce the general availability of Microsoft Fabric. Let's roll the video. Hi-powered features like co-pilot help everyone be more productive, whether it's creating data flows and pipelines riding SQL statements or building reports. And as we enter a future built on AI, you can unify, prepare, and model your data to support truly game-changing AI projects. All your data, all your teams, all in one place this is Microsoft Fabric. Yeah, it's fantastic to see, you know, Fabric, the vision come together. In fact, today, really, it's exciting to add this new capability that we call mid-airing. It's a frictionless way to add existing cloud, data warehouses, as well as databases to Fabric from Cosmos DB or Azure SQL DB, as well as Mongo and Snowflake, not only on our cloud, but any cloud to Fabric. And they're all in open source Apache Park K format and the Delta Lake format that's native to Fabric. And to bring this home, let me just kind of walk you through us.simple example, let's take an electric car charging company that wants to proactively alert its maintenance teams and crews about stations that need maintenance and servicing. So the real-time data is streaming and its IoT stuff is flowing right in from the charging stations into a Cosmos DB. They can use mirroring to keep the Cosmos DB and fabric automatically in sync. Inside of fabric, they're already connecting all the other relevant data. It could be maintenance schedules, weather from Azure Databricks, AWS, S3 or ADLS together into this one single lake house. With all this data unified, you can stand obviously model on top of it using just data in the fabric. But they can also use now this new integration between one lake and Azure AI Studio to build a preventive maintenance model that alerts maintenance teams when an EV station is likely to need servicing. And of course, you can build a simple power app that delivers these alerts to the maintenance crew. You can even embed a chat function into a power app to gather more context about the alert. This type of example is how all your data, operational store, analytics and AI all can come together. In fact, we're integrating the power of AI across the entirety of the data stack. This retrieval augmented generation of the rag pattern is core to any AI-powered application. It's what allows you to bring together your data with these foundation models. And the first thing we did is we've added vector indices to both Cosmos DB as well as to Postgres SQL. And we're in stopping there. We moved the management of AI-powered indices out of the app domain into the database itself with Azure AI extensions for Postgres SQL. This makes it easy and efficient for developers to use AI to unlock the full potential of all their relational database, all their relational data in a database. And with Azure AI search, we built a first-class vector search plus state-of-the-art re-ranking technology. Right? Delivery, this very high-quality response is much beyond what you can just get from a vanilla vector search. In fact, just last week when OpenAI moved some of their APIs, like their agent API, from a standalone vector database for chat GPT to Azure AI search, they shot unbelievable scale benefits. And it's fantastic to see this now powering chat GPT. Now let's move up the stack and talk about how we are reimagining all of the core applications in this error of AI. Let's start with Teams. Our vision for Teams has always been to bring together everything you need in one place across collaboration, chat meetings, and calling. More than 320 million people rely on Teams to stay productive and connected. It's a great milestone. Just last month, we shipped new Teams, which we reimagined for this new error of AI. New Teams is up to two times faster. Up users 50% fewer resources and can save you time and help you collaborate a lot more efficiently. And we have streamlined the user experience. It's easier to get more done fewer clicks. It's also the foundation for the next generation of AI-powered experiences transforming how we work. And with new Teams is also available across many places now. It's available on both Windows and Mac, of course on all the phone endpoints. But Teams is more than a communication and a collaboration tool. It's also a multiplayer canvas that brings together these business processes directly into the flow of your work. Today more than 2,000 apps are part of the team store apps from Adobe, Atlassian, ServiceNow, Workday have more than one million monthly active users. Companies in every industry have built a 145,000 custom line of business applications in Teams. And when we think about Teams, it's important to ground ourselves that presence is in fact that ultimate killer application. And that's what motivates us to even bring the power of mesh to Teams, reimagining the way employees come together and connect using any device, whether it's their PC, HoloLens or Metacrest. I'm excited to share that mesh will be generally available in January. It's sort of been something that we've been working on diligently behind the scenes and it's great to be bringing it like using avatars. You can express yourself with confidence whether you're joining a 2D Teams meeting or a 3D immersive space. With immersive spaces you can connect in new ways and bring discussions all into one place. With spatial audio, for example, you can experience directionality and proximity just like in the physical world. And with your own custom spaces, you can create a place tailored for your specific needs, like an employee event, training, guided tours or even internal or external product showcases. Using our no-code editor or the mesh toolkit, we are looking to see how mesh in Teams helps your employees connect in new and very meaningful ways. So we're now let's move up to the very top of the stack which is the Microsoft Copilot. Our vision is pretty straightforward. We are the Copilot company. We believe in a future where there will be a Copilot for everyone and everything you do. Microsoft Copilot is that one experience that runs across all our surfaces, understanding your context on the web, on your device, and when you're at work bringing the right skills to you when you need them. Just like say today you boot up an operating system to access applications or a browser to navigate to a website. You can invoke a Copilot to do all these activities and more to shop, to code, to analyze, to learn, to create. We want the Copilot to be everywhere you are. It starts with Search which is built into Copilot and brings the context of the web to you. Search as we know of it is changing and we are all in. Bing Chat is now Copilot. It's a standalone destination and it works wherever you are on Microsoft Edge, on Google Chrome, on Safari, as well as mobile apps coming soon to you. Our enterprise version which adds commercial data protection is also now Copilot. You simply log in with your Microsoft Entry ID to access it. It will be available at no additional cost to all eligible Entry ID users. And just two weeks ago we announced the general availability of Copilot for Microsoft 365. It can reason across the entirety of the Microsoft Graph that means all the information in your emails, calendar, meetings, chats, documents, and answer and complete tasks. It integrates Copilot into your favorite applications, whether it's Teams, Outlook, Excel, and more. And it comes with plugins for all the enterprise knowledge and actions available in the Graph. When it comes to extending Copilot, we support plugins today and we are also very excited about what OpenAI announced last week with GPDs. GPDs are a new way for anyone to create a tailored version of Chat GPD that's more helpful for very specific tasks at work or at home. And going forward, you will be able to use both plugins and GPDs in Copilot to tailor your experience. And it goes beyond that. You will of course need to tailor your Copilot for your very specific needs, your data, your workflows, as well as your security requirements. No two business processes, no two companies are going to be the same. And that's why today we're announcing Copilot Studio. You know, with Copilot Studio, you can build custom GPDs, create new plugins, orchestrate workflows, monitor, in\n",
      "[SOURCEFILE:  Putting customer success at the heart of everything—Safra Catz keynote  Oracle CloudWorld 2023.txt]Please welcome to the stage, Safra Kats. APPLAUSE Uh, this is so fantastic. Yes. Together, we can do pretty much anything. And between all of you and us, this is a truly stunning moment for all of us, for technology, for people, and for doing things that, I think many of us weren't even sure, was ever going to be possible. We get to celebrate and learn from each other. And today's keynote is really focused on all of you, on teaching and learning and sharing, and showing what's possible when you have the right technology and the right mindset. I think you're going to learn a lot. But before we start, I want to thank you, because the thousands of you, oh my goodness, the thousands of you out here have given us the simply the most precious thing, which is your time. And you are using that time to learn, to advance, and to share, and we're unbelievably grateful to you for that. We've really tried to reorient our entire company around you, around making you successful. And the fact that you're all here here in Las Vegas with us really means a lot to us. So thank you. Thank you for everything you do. So thank you. Thank you. Thank you. And I want to, of course, I want to thank our partners and our sponsors for this event. I think it's an incredibly worthwhile event. And again, sharing, learning, collaborating together. And then finally, I want to thank my employees, our team. You all have brought us here, and we are so strong together. And as you all know, it's only the beginning. So very, very exciting. And thank you to all of you. Thank you. Thank you. All of you. You make our customers successful. And it is truly amazing all I hear about is that. Now, today, in this keynote, we are going to talk a little bit about how companies handle change, how they see opportunities. And instead of shying away from them, they embrace them, how they see challenges, and they adjust to them. And they use those things as motivation to move forward and to advance and how they grab technology that is now available and change their entire way of operating. You're going to meet five executives, all of whom, are truly courageous because they saw situations changing in their industry or in their company or in the markets or even in what's possible. And they took the risk by moving to new technology and advancing even further than I think they even imagined was possible. So with that, we'll start with one of the most transformative companies that many of us ever deal with every day. And that's Uber. APPLAUSE We all know Uber today for our rights, food, easier connectivity and movement. But the company was originally founded in 2010 with a more singular focus, providing access to transportation at the touch of a button. Now, 13 years and more than 39 billion trips later, Uber is a platform that continues to solve problems around getting this closer to where we want to be, changing how people, food and things move through cities, opening the world to new possibilities, all still at the touch of a button. Now, please welcome Dara Koswishahi, CEO of Uber. Thank you, Dara. Thank you. Can you imagine? I mean, you've been CEO for six years now. Six years and look at the changes you've made. So 2024, describe the Uber of today. Well, the Uber of today now, we have undergone an incredible transformation since I joined, which is great. It's been team sports, so thanks to everyone at Uber who has been a part of that transformation. But really, as we look forward now to 2024, it's all about growth for us. We operate at pretty enormous scale. We complete about 2 billion trips a quarter. So 2 billion rides to work or a delivery of food or grocery, et cetera. And to keep growing at the 20 plus percent levels that we're talking about, we have to add a billion and a half trips every single year to the platform. And that requires us to innovate and build new products, whether it's two wheelers in South Paulo or three wheeler Ubers in New Delhi or an Uber bus service that we're creating. You can get 20, 30 people in a single bus, reduce congestion, et cetera. It's a service that we're building in Egypt or grocery delivery in Santiago. All of it is new innovation and new products that we're building. Or now you can use your words to get taxis in New York and Rome and Paris as well. All of the growth requires incredible innovation. It means building out new products and keeping that entrepreneurial mindset as we get from 23 to 24. So all that change since 2010. I mean, what were the key decisions that were made and what led you there? Well, in 2010, and even when I joined the company, Uber was all about rides, right? Push a button, get a ride. We were essentially transportation company. And for us, we decided to use the same technology ecosystem, the pricing systems that we had, the matching, the mapping, to extend transportation, transportation of people, also to transportation of food and things, right? Uber eats when I joined about six years ago was an idea that it was about 5% of the business. But now that part of the business, which is a transportation of things, is a $50 billion business, all built organically. Our Uber eats business is just as big as our Uber rides business. It has been an incredible transformation in terms of the company. COVID, while COVID, decimated our mobility business, the delivery business grew at unprecedented rates. And now we are really the go get company. Anywhere you want to go, anything you want to get, we want to be that operating system for your everyday life, push a button, and then good things come to you. I mean, what an incredible transformation. And I just wonder, I think many of the folks out here, they don't know that you sometimes get behind the car and drive around San Francisco and deliver and pick up. Tell me a little bit about what you learned when you were actually in the business on the front line. What you learned. Well, I think you know software that, in order to really understand the business, you've got to put yourself in the shoes of your customer. Right? And anyone who comes to work at Uber comes to work because of the impact that our product is having every day. And they use our product, obviously for going places and getting food. But what we saw is not as many of our customers use their product from a driver's perspective. Right? And we've got six million earners on our platform now. And they are very much the customer as well. So we've been encouraging employees, including myself, to drive it, to deliver. So if any of you are in San Francisco and you get a Tesla Model Y, it might be me. Please give me five stars. You know, I want to keep my product ready. He is a five star driver, by the way. Five stars. And for me, it's been a great experience because I think you take your driver for granted. Right? And when you're driving in a city, you're going through unfamiliar areas, you know, to make sure that the riders having the right experience, to accept the right trips, et cetera, you really get an appreciation for what it takes to be behind the wheel. And that appreciation is incredibly important when you're building product for drivers. Our roots have always been B2C, right? Business and consumer. We are a first-class consumer product. But when you look at the product that we're building for drivers, it really looks more like an enterprise product. It looks like a B2B product. Every one of these drivers, they're their own small businesses. They run their own PNLs. And the systems that you have to build, have to be there predictably, and do what these drivers expect, and then hopefully more. And that's the kind of resilience that you really need. When you are really the platform for their business. Yes. And it's really high standards. Consumers, I think, every once in a while, they kind of understand, but for a business, a lot of folks are involved. Now, you've had another very, very big change recently, because the markets changed, and then everybody, all of a sudden, love growth, we all love growth. But then started to talk about operating income and profit and all those kinds of things. Well, you just had your first quarter of gap operating income, while still explosive engagement and changing the business, all doing it simultaneously. So share a little bit about how you did this modernization. Well, also, I'd say doing more spending less. Many of you have heard me say before. And how'd you do it? 100%. So the words Uber and gap operating profit, they haven't belonged to get it historically. But they will in the future. The demands of the market are different now. And for us really balancing this growth and profitability has honestly been a skill that has been a learn skill for us as an enterprise. And some of these newer innovative products that we talked about, you know, if we build out two e-lors, for example, in Brazil, in Brazil alone are two e-lors, would represent the six largest countries in which we operate. These, the Moto Uber Moto product has grown faster than we could have ever expected. And with some of these newer products, it's impossible to predict how quickly they're going to scale, how quickly they're going to grow. So being able to modernize our back end and ecosystem and work with Oracle Cloud, for example, has allowed us to be able to scale these products in a way that we could have never before, without going through all of the, you know, trying to forecast what growth is, essentially Oracle Cloud is growing with us, right? And when we look at our growth this year and going to next year, 100% of our growth in compute is going to come on the Oracle Cloud, and that allows you to scale the right way, make sure that your unit economics stay predictable, stay exactly where you want to as you grow, and as a result, Oracle has been a big, big part of our transformation to grow, but to grow profitably as well. It means a lot to me, as I mentioned to you earlier, for me, every time I see an Uber go by, I think to myself, we're a little bit of that. And makes me just so incredibly, incredibly proud. And listen, what you really basically did is take advantage of the economies of scale and cloud, where you benefit not only from your own scale, but from the scale of really many of you in this room, and our entire global network, and then, when you need it, it's there, when you need less, you don't have to pay for it. I mean, that level of elasticity and economy of scale, we really can't beat that. You can never beat that along. Especially during uncertain times. That's for sure, especially during those. Well, there's another piece that I, and I particularly wanted you up here, because I wanted to make sure that many of you out here know about this capability, because we partner with Uber in connecting our retail point of sale system directly to the platform, to the Uber driver platform. And I think it's very important, because many of you are really struggling with some very giant competitors, and you need to benefit from the economies of scale, really, of getting products to your customers cost effectively. So maybe you could share a little bit of both the vision and what's going on. Absolutely. So one of the businesses that we're the most excited about is Uber Direct. And essentially what we've taken is, you know, Uber is powered by this incredible logistics ecosystem. It's six million drivers who are carrying people, then taking food, and, or groceries, et cetera. We have taken that logistics ecosystem, separated from our mainline stack, and we're providing it to retailers all over the world. And it allows the corner retailer to out Amazon Amazon, and not just offer next-day delivery, but it's same day on demand delivery, which is incredibly excited. Exciting to me. 3,500 brands already use Uber Direct. Apple, for example, if you order, and iPhone, you can get it, deliver that same day, we want that same delight to come to every single local retailer, and what's incredibly excited about this initiative is, the local retailers powered by Oracle retail systems now can plug into Uber Direct to offer on-demand delivery, and even on-demand returns. If something doesn't work out, we can have a career coming, pick up your returns, take it back to the store. All of it is powered through Oracle Technology and Uber Technology and Logistics. It's one of our most exciting initiatives. It's so exciting for us because we grow with you. And so what we've covered, growing in uncertain times, profitably, and expanding into all new markets using technology. I mean, how exciting is this? For all of you, Uber Direct is being highlighted in the hub, because I know many of you are really trying to solve exactly these problems, and I'm so thrilled to be partnering with you. Thank you. Thank you for working with us. My technical teams love your teams, and I feel like sometimes, and I got this from one of your folks one time. They can't tell where we end, and you start. Exactly right. That's the joy for us, so thank you. Thank you very much, Raviny. Thank you. Thank you. Thank you. Just so inspiring. Imagine growing 20% profitably, and really dealing and expanding worldwide for us, their spectacular global partner and wonderful customer, and a true test of the capabilities of our cloud. Next, I'm excited to hear from Aeon about automating sales, marketing, and service, while bringing all that data together, focusing on customer success, Aeon. As a global professional services firm, that provides a wide range of risk advisory solutions, Aeon is on a mission to equip and empower business leaders all over the world. Aeon provides clients in over 120 countries, with insights and tools to feel confident in making decisions that ultimately protect their business and enable sustainable growth. Please welcome to the stage, Laurie Goldtoman, Chief Client Officer, CEO for Enterprise Clients, for Aeon. Thank you, Laurie. Thank you for being here and for sharing. Of course, we're both basically obsessed with customer success. So you are not only the Chief Client Officer, though I feel like I am too sometimes. You're CEO of the Enterprise Clients. Again, incredible work. You're so focused on putting the customer at the center. You share with everyone else here about what that is approach means for you all. And I think they're going to learn a few things there too. Thank you so much, Safra. I love the theme of Cloud World Client Success when I listen to the Uber story. We don't feel successful unless we're helping companies like Uber be successful. And our strategy is really centered around three things. Building customer intimacy and the relationships, bringing them relevance and thought leadership to help them make decisions and then driving impact at the end of the day. So we are so pleased to work with Oracle CX. You are empowering our end-to-end client strategy. And when I think about the relationship, as we've all come out of COVID, the amount of stakeholders we really need to partner with from the head of ESG, ahead of supply chain, CHROs, the world is so complicated today that we've really got a dig deep into understanding our clients at a level I've never seen before. And then the relevance that we bring them is structured around risk capital, which is our reinsurance and commercial risk business, where we're doing everything to protect the balance sheet of companies to our human capital, which is really end-to-end talent. And the three big themes we talk about there are how do you reshape your employee mix to meet your strategy? How do you re-skill? That's a big one. And then how do you reimagine total rewards, whether it's where you want to work, or how you want to have a health and well-being strategy or safe retirement? And all of these things are empowered by moving 20,000 people of our firm into our A on business services that is the engine that powers all the operations. So Oracle CX gives us that end-to-end strategy, and we are delighted with how we can serve customers today. Thank you so much. Thank you for sharing. But to get to where you are today, you actually started with what a lot of people know of all those silos of data everywhere. How did you really integrate it? And how did you even decide to have the courage to undergo such a project? Well, we've always been such a client-centric firm. But when you think about a bent at A on for 30 years, and the amount of acquisitions and capabilities that we required, and everybody had their own special system that they said, we can't change it. We have to stay exactly as we are. And we were left with 30 regional and local CRM systems, six different global platforms. And when you wanted to serve your client as a global team, it was really challenging to do that. So we consolidated all of those platforms and all of that data into Oracle CX. It was a heavy lift. There was a lot of change management. But now we can actually serve our clients with everyday immediate action around what are their highest and most complex needs that we can serve. And we never could have done that before. We'll soon come out with our Global Risk Management Survey. And we're talking about mapping all the relationships we have around the world, Brazil, India, Asia, Europe. And when that survey comes out, we'll be able to sit down immediately and help clients think through everything from, how will you address your supply chain? How will you meet your net zero targets? If these were your priorities, what maybe can you learn from your industry peers? And what are the things in your strategy that are missing that we need to spend time on? You mentioned time in your opening. I think this is where time is so valuable. We have to make sure that we're building all of our strategies in the areas that matter. And by having our Oracle CX platform and our attention on our clients, it's possible today in a way it never has been before. And it's funny as you're talking. I'm sort of going down my own memory lane of how hard it is to bring systems together. And in many ways, it's not the technology that's hard. It's the people who say, I've got to keep what I'm using. And all of that is so much a piece of it and the change management. But you've now gone and automated sales, marketing, service. So where does this lead you moving forward? I mean, how does this impact your focus and opportunities that are now available that wouldn't have been able to do before? Yeah, I love the question. When we think about sales and marketing, today we're really thinking about the big categories of the conversations clients want to have. So that could be in workforce optimization and resilience. How do we make sure we're all ready to get through the challenges of today? It could be in digital transformation, which is what everyone in this room is an expert, certainly focused on. So we have built a data analytics service platform and housed all of our internal and external sources of data that our colleagues and consultants can now deliver on what we call one end of the spectrum, which is the brilliant basics all the way through to the more complex consulting. And the brilliant basics listening to Uber before, you've got to have proof of auto insurance or nothing moves. You've got to have proof that you have coverage when you have a loss. So by having this one platform, we can really deliver on the basics in our service. But then we can also do the more complex. One percent of employees drive 40% of the medical cost in most companies. These are your high risk claimants. So we're using predictive analytics to help people see how to avoid diabetes, cancer, and work with your primary care physicians and really building out that holistic well-being strategy. And so building a global strategy that's executed locally is something we can do now with the wonderful Oracle CX and the way we're thinking about harnessing all the data analytics together. Well, you've done the hard work in many ways. Now you've got all your data. So of course, many of the folks out here in the audience have come to Cloud World this year to hear about AI. And of course, to have the foundation, you've got to have the data. So maybe you could share a little bit about the vision for the future for using AI, for really to run the business and have better opportunities and outcomes. AI would be one of the biggest topics here and in the future for quite some time and Oracle's wonderful leadership. I know we'll all leave this conference more educated than we came. And AI for us really starts with industry. When we're sitting down with the financial institution talking about customer satisfaction, fraud protection, that's very, very different than a life sciences company who's literally using artificial intelligence to cure cancer, to take research and development all the way to medicine on the shelves and better outcomes. So we're helping them really think through how to build the skills and the staff that they need. Our data would say only 35% of companies feel they have the right skills to meet their strategy. And on AI, it's less than 5%. So while we're really helping the fast-paced discussions, we're also really thinking through the risk mitigation strategies. We've got to protect the data, the privacy, the intellectual property. I think this is an early days journey, but moving fast, but thoughtfully, I would say is the message around AI. And for A-On, it's going to make us wildly more efficient on the brilliant basics. We know it's going to help us with the insane amount of calculations we do every day with our actuaries where what would take weeks now take seconds and we know it's going to unlock innovations. So we're thrilled to be part of the AI conversation with leading firms like Oracle. And I know I have a lot to learn from all of you. Technology experts out here with AI. Well, thank you so much for us. It's exciting to see a company hyper-focused on the customer, on better outcomes, all by building the platform first to bring it all together. And that puts you in the perfect position to use the newest technology moving forward. Thank you so much, Laura. Thank you. Thank you. Thank you. So this next customer, I think many of you wouldn't have necessarily thought I would invite. And you will actually hear a story that I think is really pretty interesting. Because the way we focused Oracle is to focus on what is best for our customers. And in this case, it's an SAP enormous database that has moved to OCI. And I want to share with you, LaBla Companies. Since 1919, LaBlau has innovated the grocery retail experience to better serve Canadians. It is Canada's food and pharmacy leader, as well as its largest retailer and private sector employer. With approximately 2 billion transactions each year, in its unmatched network of nearly 2,500 stores and national e-commerce options, LaBlau brings food, pharmacy, beauty, a peril and financial services to customers through many of Canada's favorite and most trusted brands. The company's loyalty program, PC Optimum, has nearly 16 million active members and is one of Canada's largest and most loved reward programs. LaBlau's purpose is to help Canadians live life well. It makes good food affordable, health, beauty, and wellness accessible. Savings for the future possible and essential style achievable. Please welcome to the stage David Markwell, Chief Technology and Analytics Officer of LaBlauCompany. Thank you. Thank you. So David, the video showed a little bit about LaBlau's, but it's actually much broader and really mission-focused. Maybe you could share with the audience because we have lots of retailers. We've got banks. We've got people who do parts of what you do, but share the whole mission for the company and dig a little deeper than the video. Sure. Thanks, Afro. And first of all, thanks for having me. I'm super excited to be here and to share our purpose and our business with all of you. So at the heart of what we do is our purpose of helping Canadians live life well. And we do that through a set of different strategic focus areas such as retail, pharmacy, financial services, and more. At the core of our operations, though, we are a retailer. And we do that through many different formats. We have conventional grocery stores, discount stores, super stores, apparel, pharmacy, beauty boutiques. And that gives Canadians access to the products they need in a way that's convenient and affordable. Through our shoppers, drug-mart pharmacies, we offer not only prescription and services from that perspective, but we also offer enhanced medical services and additional health services and consults as we expand the scope of practice in health care. And through our President's Choice Financial Services, we give Canadians access to credit cards, savings accounts, all that support what they do from a financial service perspective every day. But where the real power lies is that we bring all this together for the purpose of helping our customers through our PC Optimum loyalty program, where customers can earn and redeem points for value across our entire network. And whether that's our digital or bricks and mortar footprint in all those formats that we described. But what really makes me excited is our focus not just on the products and services we offer, but how we operate. And we've made commitments to reduce food waste from our stores to eliminate non-recyclable plastics from our operations and supply chain and to achieve a net zero carbon footprint, all with the purpose of helping Canadians. It's really a spectacular mission and accomplishing it so well. And I think we get a little bit of insight in how you all think and work. Because if I'd say the risks you're willing to take the new things you're willing to do, I mean, listen, a lot of people still, not so many out here, but still feel like moving to the cloud is less secure or than on premise. And yet you run one of the largest SAP databases about 180 terabytes. And you successfully brought it to the Oracle Cloud. But how did you make that decision and how's it going? Sure. You embarrassed me because you said it was enormous or something. So it was like that. It is a very large database. It is. Sometimes embarrassingly so. So we've been on our cloud journey for almost six years now. I made a commitment to the organization six years ago that we would run all our applications in the cloud as we became a much more customer focused and agile business. Obviously, moving something of that size and SAP runs our entire business, all the formats I described earlier. It's the heart of everything that we do. So it wasn't without risk. We ran a very rigorous selection process. I had a few criteria that had to be met. The first were really around making sure that we had a platform in the cloud that was performant enough and cost effective enough to run at the profile we needed it to. We also needed to be able to mitigate operational and migration risk, because it doesn't come without risk to move something of that size in a very short time window. We moved it over six hours. That was our actual downtime. And then really also what I was looking for was a partner who was in it with us, and who felt as much a part of our success as we did. And so as I went through all those criteria, and there were many applicants for the role, Oracle came out on top, both from a partnership perspective as well as meeting all the technical and performance requirements. And so we've been running there for over six months, and we've certainly achieved everything we'd hoped to from both a performance perspective, our critical workloads, run much, much faster. And we've seen a meaningful improvement in batch and transactions that's unlocked a number of capabilities for our business. And going back to your first question about security, we also used the opportunity of the migration to increase our security posture from an SAP perspective to automate all our maintenance routines, and overall increase the resiliency of the platform. And all those things helped me sleep better at night. And I just really wanted to show my appreciation for Safra, because before the migration, I'd asked her for a meeting, and could she personally guarantee it would work. And she's like, text this number. And so I did. It was her number. It turns out. And she replied back. And you know, you reached out the night of the cut over the morning, and then through the hypercare, over the next couple of weeks, to make sure that I knew and that the leadership of La Blanc knew that Oracle was right there with us. And I certainly appreciated that. I have to say, we were as happy as you were with the outcome. It was truly a partnership again with you. And in a way, it also, it melts very, very well with where our hearts are, because both of us are very focused on transforming the health care ecosystem. I know many of you have heard us talk about this. But maybe you could share a bit of decision and the opportunity, sort of the crossover between health care and technology and how automation will really change that. Sure. It's a great question. And in Canada, I'm sure like most parts of the world, Canadians suffer with access to not just health care, but health information. And so we've made it a part of our purpose to leverage our vast network to deliver health care services to Canadians and to make health care information more accessible. And while we do that, Canadians trust us with their data and want to make sure that it's secure, that it's accessible, and that our experiences are personalized to that. And so we have a number of health outcomes or health journeys that we've enabled around medical adherence, where we can ensure that patients stay compliant to their medication through reminders and through specialized care. For chronic patients, we have personalized messaging that goes out to them to help them stay on their programs. And then we have our PC Health app, which allows customers to link their biometric information, blood pressure readings, diet, health care, prescriptions all into one app that allows them, along with care journeys and other wellness programs to help them really manage their health care. And this is really part of our strategic imperative around our connected health care ecosystem as we really try and help move the health care situation along in Canada. I mean, thank you, David, for sharing, because what it shows is that as you move to a very performant, cost-effective foundation, you can then expand into new areas that are critical really for the success of the business and for your country, which is amazing. So thank you. Thank you so much. Thank you. Thank you for everything. Thank you. It is really inspiring to hear about what our customers are doing. Our next customer is actually Tim Brazil. And they decided to move not a little of their business, not half their business, but all their operations to the cloud. And you're going to hear about that next. Tim Brazil is a telecommunications company that serves Brazil under the mission to evolve together with courage and transform technology into freedom. The company is recognized for leading important market movements since the beginning of its operations and standing at the forefront. of digital transformation in Brazil, helping enable innovation in agribusiness and other impactful initiatives. Tim remains a leader in 4G coverage in Brazil and a pioneer in 5G activation networks. Welcome to the stage, Leonardo Capdevil, Chief Technology Information Officer for Tim Brazil. Okay, I'm not going to let it go even one second. So, all right, you're on premise and you decide we're moving everything, not a little, not half like I told you, everything to the cloud. You have to share with everyone how you did that because you were the first communications company in the world and you've got 60, what, 62 million customers and boom, move the entire thing to the cloud. You've got to tell this story. Thank you. Thank you. We are very honored about this history because we have to remember that when we take this decision was in 2020. We were in the early stage of the COVID pandemic. So imagine all the uncertainty that we had at that time. But at the same time, we look ahead and we see that the next three years it will be the most critical and transformational for our history as a company because we were finalizing one M&A discussion to consolidate the market in Brazil acquiring a new company with a lot of assets back from the powers, including 60 million customers. After that, we were discussing about the five-deoction, what it means a lot of opportunity but some kind of obligations. And looking at that, we see that we need a very, very strong balance sheet to do all this investment. So we have a very interesting statement as our proposed that is evolve with cottage, transforming technology into freedom. So we decided to move on. And when you look at the results after these three years, it was fantastic. For example, for the M&A, after it should be approved, in just one year, we integrated the both company, all the assets, including migrating all these 16 million subscribers. Just one year, imagine that without all the facilities that we have in the cloud, it will be impossible. Second, we worked the first company to launch the five-digit standalone in Brazil. And until now, we are the leader in this technology. Again, without all the capacity that we have in the cloud environment, it will be impossible. The third is about the financial constraint. When we start the mutual clouds, we expect some benefits. But in the end, we have a head of that. We reduced our TCO in 19th-restructural in more than 30%. 30%. Yes, it was fantastic. But last and not least, we improved the performance on the MACE systems. So for example, in CRM, that is CBIO-based, we reduced the answer time in more than 50%. We improved our dealing performance, we improved our SAP performance too. So it was fantastic. It was good to have this kind of courage on that time. I mean, incredible. Think about it. It's COVID. You've got 5G rolling. You've got acquisitions and all sorts of things going on simultaneously. You move everything to the cloud and you have a TCO improvement of 30%, which we would all kill for, and ultimately, better performance. So you serve your customers better. I mean, truly incredible. And yet, tell a little bit about some of the strategies and some of the challenges of securing so much the personal data and all sorts of data. Yes, you said well, we were the first to do that. So we have to learn with ourselves. So when we started this movement, I guess that the planning was the key. So we worked together with our providers. And since the beginning, we involved some kind of a special stakeholders like cyber security, league out team compliance to guarantee that all the risks was evaluated and mitigated. Most of the most important in challenging was to change the mindset of our team. The mindset is the key point. And what we show is that, okay, guys, we know that now we are operating our own data center. But we will move ahead and we will manage our ecosystem on cloud-based. Imagine all the opportunities that we have there. So I guess that this was what moved the team for this new kind of skill. I mean, you think about it. This was really courageous. But what you said was, again, what I've always believed, it's about the mindset, it's about the courage, it's about the leadership. Because you really wouldn't be where you are now, if you hadn't done it, you wouldn't have the capacity, you wouldn't be able to move forward the way you're doing it. And it really is a testament to your team. And it's also, of course, been a joy working with you all. And you also leveraged one of our newest and closest partners, Microsoft. And the work we were doing, again, we feel like everything's changed at Oracle. Because now, again, focusing on what you need, we realize bringing Microsoft in working closely with us is going to meet your needs. Maybe you could share sort of your thoughts about how that whole partnership has worked and what it meant to you guys. Sure. It was a surprise, I should say a gift. Because at the beginning, when we were evaluated about the cloud providers, each one has their own capacity and speciality. So, for example, when you look at our CRM, it's CBO-based. When you look at other systems, it was based on Windows OS. Some systems was more database oriented, other was frontend. So how to build the best of that? When we received the answer, confirming that Microsoft and Oracle was together, was like, wow, now we can choose the best of each technology and to apply that in our system. So, it was amazing, was amazing. And one important thing here, and I'd like to make a special thanks for Oracle Team Brazil, and that is led by Luis Meisler, is that Oracle was focused beyond the technology on the customer needs. And it was make the difference, totally. Yeah, I appreciate that so much. And thanks for your effort to do that. We were the first case in the region, so we are very proud about that, Julie. And we are so proud to be part of it. And in a moment, I'm going to show you something Luis Meisler actually sent me when you all were done. So everyone will see how much it was fried for us,\n",
      "[SOURCEFILE:  Building the more intelligent future of cloud Clay Magouyrk keynote  Oracle CloudWorld 2023.txt]. Please welcome to the stage, Clay McGwirk. First I want to say thank you all for being here. I know that it's a busy time in everybody's lives and it takes a lot to come out to an in-person conference and listen to a lot of people talk about technology. Just in case you were wondering, we will talk about AI today. I know you probably haven't had enough conversations about AI, so we'll get there, but it's not the only thing we're going to talk about. I have a lot of conversations with our customers, our partners, even my employees. And a lot of what I talk about are the new things that we're doing at Oracle Cloud. And that's what we're really going to focus on here today. But what I find helps first is to really establish a baseline about what are the special properties of the Oracle Cloud. And given the recent advancements that we have in AI, one of the ways that we can try to understand the Oracle Cloud better is by asking a generative AI service. I feel like that's something that we all do these days. You go, you spin up your favorite version, whether it be from vendor A or vendor B, and you type in your question. And what's amazing is that this technology that now seems almost commonplace a year ago was really unheard of. And so as you can see, generative AI answers the impression incredibly well. But if I can't add any color to that, I'm probably in the wrong job. So I'm going to take some time to explain the special properties about the Oracle Cloud. I've been in the cloud industry for a long time. And the thing that I focus on is that the Cloud means different things to different people. We all use the Cloud every day, sometimes in our personal lives, oftentimes in our jobs. And based on who we are, the Cloud means different things to us. But in terms of common properties, the Cloud is really a collection of services that are delivered to you over a network. And to me, the most important part is that it's somebody else's job to deal with the unfun parts. And across the industry, there are different Cloud providers. There's a really different Cloud verticals, right, across infrastructure, things like enterprise applications, industry applications. And each of those Clouds are designed to solve different business needs. A typical Cloud strategy consists of separate application and infrastructure layers. And there's a cohesiveness that is missing between those two layers. If you think about the evolution of this business, Cloud applications came first, really starting in the early 2000s, and involved in parallel with Cloud infrastructure, which came a few years later. At Oracle, we have a fundamentally different approach to the Cloud. Our strategy is not to deliver separate Clouds across separate layers, but instead to combine the functionality of infrastructure, enterprise applications, and industry applications into a single unified Cloud. To do that, we've re-engineered how our applications work across all layers of the stack. So I'll tell you a bit more about some of the benefits of a unified Cloud strategy. The first is that you get cohesiveness and deployment and choice. Across our infrastructure and all of our applications, you can receive them in the public Cloud, in government-specific Clouds, across sovereign deployments, some dedicated to national security needs, and then customers can receive the same unified Cloud environment in their data center via our dedicated region offering. We also have an integrated extensibility platform. Now, there's too many things to talk about, but I'll give a couple of examples about what that means. Take, for example, our Fusion and NetSuite Analytics warehouse. This is something that takes your amazing business data, it's authored in Fusion or NetSuite, combines it using Oracle's data platform, using the autonomous data warehouse, and then allows you to supplement that with your own custom application data, other business data. That kind of thing is only possible because we offer a unified Cloud experience. We also offer, for example, our Oracle Integration Cloud that is used by essentially almost all of our Fusion customers so they can create custom extensions to their application platform. All of that sits in the same unified Cloud environment. There's also something that I hear the most from customers is that there's a huge importance around unified governance and control model. Whether it be our Cloud Guard services, things like maximum security zones, the work that we do in our identity services, our entire compliance portfolio. When customers come to our Cloud platform, all of that is unified across the entire stack. I'll give you a single example of where this is really impacting the conversations that we're having with customers. If you listen to the things that Mike Cecilia talks about in some of our industry applications, what Larry talks about with some of our healthcare needs, we go and we have conversations with entire countries about actually revolutionizing their digital infrastructure. That's from the basic infrastructure layer up through telecommunications into patient monitoring systems. The fact that we can go in and deliver that, start small and grow as the needs grow in a unified whole is just something that most other Cloud providers can't even think about. It's great that you have a unified Cloud, but I think we all recognize that that's only really valuable if the individual pieces are great. We've been hard at work across Oracle Cloud infrastructure to ensure that we have an amazing offering that underpins everything that you need. Over the past year, we've added many services like our Q-Line service, our data lake house, container instances, serverless Kubernetes, and many more. We've also added more than 13 regions and we're up to 64 customer facing regions around the world and we continue to grow our customer base every day. Here are some examples of customers that are using OCI. Subaru is using simulations to ensure their vehicle safety. True digital surgery is controlling robots for microsurgery and the experience has their data lake on OCI. According to our enterprise and industry applications, I find that oftentimes when I'm talking to specifically an infrastructure audience, there's a lack of understanding about the true breadth and depth of Oracle's applications. Whether it be in pillars like financials, human resources or supply chain, or industries like banking, energy and water, retail or telecommunications, Oracle has a comprehensive portfolio to meet those business needs and all of that runs on OCI. And the great thing is that those customers get all the benefits of the applications as well as the benefits of our cloud infrastructure. Some examples of our great application customers. FedEx runs their entire logistics business on FUSION, so far uses NetSuite to serve hundreds of thousands of their customers. And WAGAMAMA uses industry applications for restaurant ordering, service and payment processing. So the first special property I want you to understand is that Oracle Cloud is about delivering a unified cloud combining applications and infrastructure. Across the industry, the demand for compute has never been higher and has never been growing faster. As of today, there are more than 8,000 data centers around the world. Each electricity consumption of 3% is projected to grow to 8% by 2030. And CO2 emissions from data centers is projected to grow from today's 2% to 14% of the total world's emissions by 2040. At the same time, we're having this growth, energy is becoming more expensive and also less available. So as a technology industry, what should we do about this? The good thing is that every new generation of technology, every incremental step, has been improving our compute power for water of electricity. We've been making incremental improvements over the past four decades. You should go and do the calculation to see if we had the energy efficiency of computers in 1970, how much that would be impacted with half of the world? You need to turn your iPhone calculator sideways. But at the same time that we're seeing these incremental improvements, technology comes along that has a step function shift that can make a big difference, more so than just the year-over-year incremental improvement. Arm is one of those technology shifts. If you all look at what's in your pockets, whether it be a phone or a tablet, your laptops, there's a reason that the arm architecture has come to dominate our personal computing devices. And that's because energy consumption is so important in those devices. Well, the same thing that's happened on the personal device side is now becoming even more important on the server side. So at Oracle, we decided to push forward and really try to over-invest, move faster than the industry is going towards such a technology. And one of the things that we've been working on for the past year, and I'm very proud to share with everyone, is that as of today, 95% of all OCI services and all new fusion customers run on Ampere technology. The reason this is good for Oracle as well, we get received substantial energy savings. We obviously reduce our power bill. But at the same time, for every single rack that we deploy on Ampere compared to the alternative, we save the carbon footprint that could fly this entire room to Singapore and back twice. So to talk a bit more about what's going on with Ampere and why it's so impactful, please help me welcome to the stage Renee James, the CEO of Ampere. Renee, thanks for having me. My favorite topic. Well, as you and I have talked about many times, I know you're passionate about Ampere, and you're also very passionate about sustainability. You've been in the technology industry for a while. You've been working on semiconductors for most of your career. But then five years ago, you decided to go and start a new company. What motivated you to go out and do that? Yes, I have been in semiconductors for a long time. I am tired career actually. There were two twin challenges really facing us that I felt like there was an opportunity for a new company to emerge that were going off the twin challenges of the cloud, which is increased performance, linear scaling performance, and efficiency and sustainability. That quadrant of computing, which is higher performance at lower power versus lower performance, slower power, hadn't really been pioneered. The cloud opened up an entirely new opportunity from a software perspective. Because so many customers like you all were moving to the cloud, because so many of the workloads were in the cloud, we could build a cloud native processor that was very tuned to the needs of people like OCI and could deliver in addition the sustainability that's absolutely requirement. This was five plus years ago. Now with gender to the AI, that efficiency requirement is even higher. Absolutely. At Oracle ourselves, we're experiencing a lot of growth. As you can imagine, there's a lot of success happening across the company. As that growth is happening, sometimes you run limitations. You don't always get the power of the cloud. One of the reasons we shifted so quickly to adopt Ampere technology is because of that power efficiency, it allows us to continue growing our business without bottlenecks. I'm very appreciative of you guys' adoption, what you call A1, which is the Ampere Altar line. The power efficiency has been excellent, but now we're seeing in our new generations, and as we go forward, incredible performance gains as well. I'm very pleased with that evolution. The cloud is unfolding. The challenge is that five years ago, frankly, when I talked about efficiency and sustainability and microprocessors, because we used for decades, for those of you that don't know, we used to use power as a proxy for performance. We would just add more power, because if it was four or five hundred watts who cared, you could not air cool a data center that was full of microprocessors and or GPUs that are five hundred watts. There's no possibility that that's a long-term solution, given that most people are being constrained on the grid about how much more power, in fact, some jurisdictions are actually saying you can't put more data centers in. You're going to have to get more density. The approach that we went after was this dense, single core scale-out approach in a very, very power-efficient way. As a software person at heart, I'm always impressed by how you can have fundamentally different approaches that result in different options. I think the work that you've been doing at AMP here is impressive. To hear a bit more about what we're seeing, not just as an oracle as a customer, but also from some external customers, I think we should bring Medi from 8x8 out. What do you think? I think great. Hi. So, Medi, we've been working together for a while. 8x8 is a company that cares about performance, efficiency and sustainability. Can you share with people what is it that 8x8 is working on and why performance and efficiency is so important to you? Absolutely. Our journey with OCI, in fact, started on the foundation of performance and efficiency. At 8x8, we're in the business of cloud communication services, providing businesses globally with contact-center solution, unified communications, video and voice on a single integrated platform. And for our type of services, we're dealing with a real-time audio and video over the public internet. It's one of the most difficult applications to be delivered over the public internet. So we absolutely create, it's absolutely critical for us to have excellent performance on the compute and on the network side. So very, very important to us. If you guys want to know the whole story, just search 8x8 oracle, you're going to see a lot of articles on Oracle's website, on Forbes, other places, how the journey started, but to summarize it, at the beginning of pandemic, OCI was one of the main reasons that we were able to continue providing free meeting services to those who needed desperately at the time. It was a very important time for us to be able to extend our reach and provide these services to hospitals, schools, we had so many great messages coming in. We moved our services to OCI, massive amount of data transfer, if you can't imagine, millions of users were utilizing our services, were free at the time. And we saved 80% on network egress costs, which was humongous for us. And we gained 25% performance per note as we were using. But our story just did not stop there, right? It just continued. And the cycle of performance and efficiency is something that I've seen with OCI. Continuously, we're talking a lot about these things. For example, when the flex shapes were introduced, I think it opened up a new dimension actually in performance and cost efficiency. It was really great. And I believe right now with the Ampere processors, you have taken it now to a completely next level. And the other side of things, network is also super important to us. We're expanding globally. It's been amazing just in the past few years how many regions you have opened up. We have our own global reach technology. So we can deliver the best quality of service to users around the world on the single platform. So contact sensor users might be all around the world. We want to make sure the best quality voice. And to do that, we need infrastructure in the region. So we keep the media and signaling within the region and reduce the latency and the distance you traverse over the internet to get to the end users. So we provided a utilizing your infrastructure. We have been able to reach our customers really well and provide that excellent quality of service. Lastly, as far as the experience with your team, I can tell that it's an amazing team. You have a customer obsessed team. Let me tell you this. Like, literally the way that we're dealing with your product teams, they're listening to us. They're all to us internally dealing with our product teams at 8 by 8. And our account team and several guys are so integrated with our team sometimes I forget our part of Oracle. So that's great and keep up the great job. So, so many. I know you recently adopted our Ampere A1 offering. Can you tell people a little bit about what that experience was like? Was it easy? Was it hard? How did it go? Oh, great question. So let me tell you guys a story around what five weeks ago a clay calls me. We're on a meeting and tells me, Maddie, let me tell you something. We have these awesome Ampere processors that are cost effective. They're performing really well and they consume much less power versus they're great for the environment. And I know you care about all these three. We have moved a lot of our own managed services to them. Do you want to give it a try? And I'm like, absolutely. Let's give it a try. And this is the first time we have been X86 all along. So for us, let's see how it goes. Three weeks later, I'm sending you an email saying that, hey, we're live on A1's in production right now. And we have production, video traffic on this. And as far as the experience, it was actually much easier than what I thought. I threw this to one of our most agile, innovative teams our video team. And I said, hey, let's try this out. Literally, they had to just recompile one of the modules, which was native. We were able to find all the libraries that we needed, including a security module that was like a mouse from our security team. That was available. So we put it on, tested it. The rest of the time was really working on the orchestration and switching the compute shapes that we're using and all of that. We went live. I waited a week actually to make sure it's working really well. And then actually I notified you guys. And the performance, these processors, they don't do hyper threading. It's very consistent linear performance that we have experienced. Initially, we were conservative the way that we utilized the number of processors that we assigned. Then we started to make the machines one harder. What if that, I mean, like putting more loads on the machine? Because the temperature or lower actually on these machines. Yeah, thank you. Yes. They don't get hot. Yeah, exactly. And we're like, oh, they're performing really consistent, really good. So that linear scalability and performance has been really good. And I'm sure you're happy because your density in the data centers are getting better with these. So look, this is a great, you're happy. I'm happy. Everybody's happy. I'm happy. Excellent. So Renee, we're here. We hear a lot about all the great things that are currently available. And the advantages of using the current technology. How do you see this continuing to evolve? Yeah, that's a great question. Well, first of all, thank you. We've been an eight-by-customer since I started the company. So now I'm happy to be able to. And I'm pure customer. We're excited. I mean, running Fusion and the database and applications of that, Elk, are, you know, profound for architecture to move to that point. The next product is called Ampere 1. We named it that because it's the first one of our own cores that we've designed from the ground up. And have a lot, of course, more cores and more performance, but some very unique features that are important for cloud providers like you all. Well, and I'm very excited to say today that, you know, along with the new Ampere 1 processor, the OCI will be launching our new A2 instance early next year, which will make this great new technology available to all of our customers worldwide. Renee, Mette, this has been awesome. I really appreciate you coming here and telling everybody about your experience. Thank you very much. Thank you. Thank you very much. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you, Renee and Mette, on the advantages of these processors. But it's also important, during this transition, to understand that there's a need for a win-win. We each have a role to play. You, as a customer, must adopt this technology, reduce your energy consumption, and therefore help the environment. It's our responsibility as a cloud provider to incentivize that by making sure that the more efficient option is also more cost-effective for you. And at Oracle, we are committed to delivering that sustainable cloud. And we're committed to passing those savings on to you so that you are incentivized to be more sustainable. That's another very special property of the Oracle Cloud. I talk to customers a lot. And one of the most common things that they bring up over the past few years is that they have multiple clouds. And what they ask from us, as a cloud provider, is to make sure that our clouds work well together. It's an important part of our strategy at Oracle that we ensure our cloud works well with others. And in the pursuit of that vision, we have multiple offerings that I'm going to talk about, the first of which is Oracle Alloy. Oracle Alloy is a set of functionality that enables others to become cloud providers themselves. We launched this at Open Cloud World last year. And since then, we have two very important partners in customers, Telecom Italia and NRI. Has been a long time dedicated region customer of ours. They helped us evolve our products, they're an amazing partner and give us great feedback and help us deliver the solution together. And once they understood the power of alloy, we were, they immediately decided to take that offering. And together, we can go and serve more highly regulated markets like the Japanese government market. Another thing that we've been working on for many years is our relationship between OCI and Azure. And the reason for that is because, as you can imagine, Microsoft and Oracle, we have a lot of joint customers. If you go to any on-premise data center, there's a lot of Microsoft technology and there's a lot of Oracle technology. More than four years ago, we started with our first interconnect between OCI and Azure. And since then, we've launched more than 12 regions around the world. The focus of that product was really on making it interoperable at the network layer, at the identity layer, and having a shared support model. Along that path, we also got a lot of feedback from customers. They liked the fact that we were working together. They liked that we were trying to really solve their joint problems, but they wanted more for most. And so a year ago, we launched our Oracle Database Service for Azure, which was really an integrated experience focused on, how can you use the best of Oracle database technology? Exadata, autonomous database, heat wave, and use that in a really easy to use interface with your existing Azure applications. Since then, the cross the interconnect, across our Oracle Database Services for Azure, we have almost 500 customers around the world. And not only are we seeing continued growth, we're seeing an acceleration in the adoption of this technology, but we're not standing still. Last week, Larry and Satya announced an expanded partnership between Oracle and Microsoft. And that expanded partnership is called Oracle Database Azure. And to hear a little bit more about that, and to hear about some of the progress so far, even though it's only been a few days, I'm gonna invite Juddson Alltoff, Executive Vice President and Chief Commercial Officer of Microsoft at the stage. Thanks for being here. Thanks for having me. So. Last week, Larry and Satya were hanging out in Redmond. It went well from my perspective. ours as well. It good. It was nice to host Larry in Redmond for the first time ever. It's been a few days since then, not too long, but you know, we're not here. But we're still here. We're still here. We're still here. We're still here. We're still here. We're still here. We're still here. We're still here. We're still here. We're still here. But you know, you talked to customers all the time. What's the reaction that you're seeing from customers? Yeah, I mean, first of all, look, we couldn't be more excited about this partnership play. And for me, both personally and professionally, a guy up until about four years ago, I thought the only time I would ever see the Oracle logo and the Microsoft logo together would be on my LinkedIn profile. And I was talking to my daughter the other night and telling her about all the work we're doing. And she reminded me about how she and her brother grew up with Oracle One Seas on. And like, you know, had more Oracle logo where than granables. But so aside from the personal nostalgia, customers are fired up about what we're doing. Because as you talked about, look, there's no such thing as like a mono cloud strategy in this day and age. Customers want us to work together. They want to see the ability to leverage applications running on Azure connected to the Oracle database service. And frankly, without limits, without boundaries. And AI is only gonna further build upon that. Every single customer that comes to Microsoft talking about AI is a customer that we counsel and saying, look, you can invest all you want in your AI tech. But if you don't first ground it in your data estate, all you're gonna do with your AI is make mistakes with greater confidence than ever before. So look, the importance of being able to bring the Oracle data estate, exadata, the autonomous database, connected to Azure and help our customers accelerate progress. It's an awesome, awesome opportunity. Yeah, that aligns with exactly what I hear from customers all the time. There's, this has clearly been an area that customers have wanted us to work on. And we have been working on it for several years. What do you think about this offering that is, what makes it different than say the stuff that we did with the interconnect or even some of the other stuff we launched last year? Well, I like the timeline that you showed, because look, as you stated, we've been working on this for a while. Our engineering teams have been, side by side, hands on keyboard, trying to create better experiences for customers now. And in the interconnect was revolutionary. It was really the first time two major cloud providers came together to deliver a better experience for customers and earnest. And we learned a lot as you stated. And the bottom line is the kind of service that a customer expects to come from Microsoft and from Oracle is a high performance real-time experience. And so the pressure on the interconnect was always the latency. And gosh, could we get you all to coordinate a little bit more effectively on data center locations? So we really thought the best thing to do was to actually put OCI inside of Azure, so that customers can have that seamless experience without boundaries. So you can literally go into the Azure portal. If you have a commitment to Microsoft for what we call an Azure Cloud Consumption commitment, you can literally retire that commitment through leveraging Oracle database services that are connected right in the same disk and a rack, side by side, zero latency, the best of both worlds. So we think it's a phenomenal leap forward. As you stated, we're starting off in 12 regions, but we hope to grow it from there. There are hero regions, by the way, where we have the bulk of our compute and the bulk of our AI capabilities today. And so as customers roll out more and more, generally, AI solutions leveraging Microsoft co-pilates and some of the demos that you all have shown with Power BI connecting into the Oracle database, they can expect a far better experience than ever before. Yeah. So you've got a lot of excited customers. We've got a lot of excited customers. What advice would you give to those customers? How do they take that next step? What do they go from here? I mean, the times now we're open for business. We have a lot of customers already in pipeline, as even I have talked about. This was one of those announcements where we didn't have to do a lot of arm twisting to get customers to step forward. I think the first reaction was finally the most common reaction anyway. So look, reach out to your Oracle sales team, your Microsoft sales team. They're both equally incented to work with you on all of this. Failing that, you can reach out to me at JudsonatMicrosoft.com. Like I am thrilled about this partnership. And I'm thrilled about what we're going to do together with customers in market. And look, this is just the beginning in terms of the unlock. I think we can really have. I'll come back to this AI point. Look, every customer I talk to has 100 big ideas of what they think they can do with AI. And I guarantee you, it won't be unlocked any faster than working with Oracle and Microsoft together. So we're thrilled, Clay. Well, Judson, this has been a long journey for both of us. I know you and I have worked on this together for quite some time. It's great to have a moment where it's real and live. And I appreciate you coming here today and talking about it. Thank you very much. Thanks so much for the partnership. Yes. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. So as you heard from Judson, this new partnership is really about a few things. It's about reducing the latency for these workloads so that you can take your most demanding workloads through both of our clouds. It's about a unified user experience. And it's also about a unified commercial experience where you can procure this through the marketplace and receive private offers from Oracle. We can't be more excited about it. And I think that not only is this going to be very impactful for customers, I think this is something that will continue to evolve the cloud industry going forward. Another part of our multi-cloud strategy is the work that we're doing with heat waves. MySQL is an extremely important technology to work with. And we've invested very heavily into our MySQL Heat Wave service. And we believe it's the best play to run an open-source database in any cloud. And so another part of that MySQL Heat Wave strategy is that it's critical that we meet our customers where they are. To learn more about the great things that we're doing with MySQL Heat Wave, I'm going to invite Edward Screven, an executive vice president and chief corporate architect at the stage to talk more about our Heat Wave strategy, availability, and some adoption by our customers. So we heard Clay talk about improving efficiency. So using less power, saving money, we all know how important that is. And doing it without actually sacrificing performance, running it released as fast as you did using older hardware. So we can use new infrastructure. We can be more efficient, use less power, save money. But what if you could improve query performance by 10 times? What if you could train your ML models 25 times faster? What if you could replace 5 AWS services with one service? And still run an AWS by the way, because you can choose your own cloud. Well, you can do that with MySQL Heat Wave. So if you don't know, MySQL Heat Wave is a database service that combines online transaction processing with lightning fast analytics. The only other database service I know that does that is actually the Oracle database service. But using autonomous database, using autonomous data warehouse, you can do both transaction processing and high speed analytics. The advantage that are clear, you don't need to move the data. If you use other cloud database services, you have a transaction processing system. You want to do analytics on it. You have to create ETL to move the data from transaction processing system to analytics system. So not only do you pay more, because you have to pay for transaction processing and analytics service, you have to move the data, which means the analytics you're running, they're not real time. Using MySQL Heat Wave, your analytics are real time. You get the answers you need in real time. Now, one very distinguishing characteristic of MySQL Heat Wave, it has built-in automatic machine learning. You don't need to use a separate service to do machine learning. You're using AWS, you're probably using Sage if you wanted to machine learning. You don't need to use a separate service for machine learning if you use MySQL Heat Wave. And that machine learning is automatic. And what I mean by that is, you just tell MySQL Heat Wave, what is your goal? I want to do regression, I want to do classification, I want to do recommendation. You show the data, you show the data, you say, I want these tables, these columns, and say, go. It chooses the specific machine learning algorithm, and it chooses the hyper parameters for you. It then trains the model for you. It manages that entire process. You don't need to do data scientists, you don't need to manage the data. And not always at lower labor is also a more secure. You're not moving the data around. The data stays in the MySQL Heat Wave database. Now, the summer, we launched something called MySQL Heat Wave Lakehouse. MySQL Heat Wave can now query data stored in files and object store. Files that may be generated by IoT devices, files that are generated as logs from various services you're running. Files that are generated as backups from other database services. So for example, if I'm using Aurora in AWS, I can take an Aurora backup and query it directly, directly using MySQL Heat Wave Lakehouse. That means MySQL Heat Wave is an analytic solution for more than just MySQL applications. If you can generate files with data, you can query it with MySQL Heat Wave. All of this is managed through something we call autopilot. Autopilot is machine learning driven automation of MySQL. You don't need to tune MySQL Heat Wave. It's automatically tuned. You don't need to worry about how to scale MySQL Heat Wave. It's automatically scaled. You don't need to be an expert in database at all to use MySQL Heat Wave. And the result of autopilot is that we have achieved amazing performance, dramatic performance. So here I've got a slide that's comparing MySQL Heat Wave versus several other databases you might be using for analytics might be considering using. The price performance advantages are overwhelming. This is a 500 terabytes, 500 terabyte, DCPH benchmark. The price performance difference with Redshift eight times. Eight times. The price performance advantage against Databricks 18 times. Snowflake 22 times. Google BigQuery 30 times. Now these are price performance numbers. If I showed you just the straight performance numbers, they would be equally dramatic. MySQL Heat Wave is much faster than these other dedicated, these other dedicated, analytic databases. And remember, you get it together with OLTP capability. You get it together with lake house capability. Now what that means is you get cost savings, you get energy savings, you get faster insights, and you can get all of those things on your choice of cloud. Of course, MySQL Heat Wave runs on OCI. It runs on AWS. If you have files you're generating into S3 buckets, you can query those directly. So MySQL Heat Wave Lake House without moving the data out of AWS because MySQL Heat Wave runs directly in AWS. And of course, you can also access MySQL Heat Wave from Azure. Now, why don't we now talk to a question? Why don't we talk to an actual question?  my single heatwave customer in video and I'd like to please invite Chris May onto this stage. Chris, Chris, I'm sorry. Chris, Chris, I'm the head of my team Blackwater and Cloud Engineering at in video. Thank you, Henry. Chris, why don't you start by talking a little bit about what you're doing with my single heatwave and the problems you're trying to solve? My team provides critical services that support all of the engineering and operations work that keeps in video working. One of the specific areas were challenges that we addressed had to deal with our code coverage application. We needed to make sure that we had accurate reports on what percentage of our code was being executed and exercised by our test suites. Right. So just to make sure I understand. So basically, the engineers at Nvidia are writing both software and hardware code, probably system varialog for your chips and you need to verify that the tests that you're running are covering all of that code. Exactly. Right. Now, I think many people here are probably software oriented and so we're used to being able to just like fix the bug and deliver a patch. Like, if you actually have a bug in the varialog, can you patch those chips? That is a great question and yeah, there are work arounds that our hardware engineers can do. But it's very difficult. It is very expensive. And usually you need to know early on before you actually tape out. So this is a critical part of the way Nvidia does business. And so do you have any metrics that have shown improvements with using my single heatwave? Yeah. So would we move our coverage application to the cloud? We identified a specific area of incompatibility that relied on a database engine that we could only support on prem. And that was challenging. We were introduced to heatwave as a possible solution and as we delved into that, we realized that the in-memory analytics that's built into heatwave perfectly matches our need for that piece of it. So that reduced a lot of the code complexity around this and we had some phenomenal results from that. And so that reducing that code complexity, I mean, so you got not not only ran faster, but you now have an environment that's probably a lot more maintainable. Yeah. Exactly. So I just freed up smart engineers to work on other problems than maintaining this code. Some of the other benefits that we ran into, we noticed a 50% increase in performance of our queries. And this is both for the analytics queries as well as our normal transactional work. That's great. That's great. So I understand you're also using heatwave for machine learning now. Is that right?\n",
      "[SOURCEFILE:  AWS reInvent 2023 - Keynote with Dr Swami Sivasubramanian.txt]Please welcome the Vice President of Data & Ai at AWS. Hello everyone, welcome to Readman 2023. This year I'm especially excited to stand on this stage and share our vision with all of you. That's because this year we are standing at the edge of another technological era and era in which a powerful relationship between humans and technology is unfolding right before us. Generative AI is augmenting human productivity in many unexpected ways while also fueling human intelligence and creativity. This relationship in which both humans and AI for new innovations is right with so many possibilities. This partnership is similar to many symbiotic relationships we observe in nature. There are different species not only co-excessed together but also benefit from one another like we see with the whale sharks and the Ramora fish. whale sharks offer the fish protection from its predators while the fish keeps the shark clean and healthy. There are even symbiotic stars that share gases and heat occasionally producing a beautiful explosion of energy called a supernova. Today the world is captivated by Generative AI's potential to reenact the way we work and form new innovations. Our own supernovae if you will. But the exploration of the symbiotic relationship with humans and technology is not really new. In fact over the last 200 years mathematicians, computer scientists and many visionaries have dedicated their entire lives to inventing new technologies that have really reduced manual labor and automated many complex human tasks. From new machines for mathematical computation and big data processing, to new architectures and algorithms to recognize patterns and make predictions, to new programming languages that made it significantly easier for people to work with data. One of those visionaries was Ada Lovelace, who's also known as the world's first computer programmer. Ada is often recognized for a work of Charles Babage on the analytical engine, one of the earliest versions of computer in the 1830s. Like Babage Ada believed that computers could remove the heavy lifting from many computational and analytical tasks. But what said Ada apart from many of our contemporaries at her time was her ability to recognize the potential of computers that they go way beyond just glorified number crunching. She discovered that they could be programmed to read symbols and perform a logical sequence of operations. This discovery marked a shift towards using computers for complex tasks. Ada also speculated that these computers could understand musical notations and they could even create music in the future. This could be what called an early ode to the potential of generative AI. But despite her belief in their potential, she made one thing really, really clear. These machines could not really originate anything. They could only generate outputs or perform tasks that humans were capable of ordering them to do. True creativity and intelligence she argued only originates from humans. In this way, humans and computers would have a mutually beneficial relationship with each contributing their own unique strengths. Personally, Ada's analysis of computing is really inspiring to me. Not really because her works do the test of time, but also because I have a young daughter instead. In fact, Ada is one of my daughter's favorite people from the book called Good Night Stories for Rebel Girls. So if you ask my daughter, she will tell you that I have a career right now because of Ada, which I think is really cool. Now today, it's clear that Ada's contributions are gaining even more relevance as the world of generative AI unfolds. While ML and AI has spurred rapid innovation over the last couple of decades, I believe this new era of human-led creativity with Chen AI will shift our relationship with technology towards one of intelligence augmentation. I find this core evolution of humans and technology incredibly exciting because that is also under the symbiotic relationship happening under the hood that is key to driving our progress forward here. The relationship between data and Chen AI. The massive explosion of data has enabled these foundational models to exist in these end of first place. The large language models that power generative AI to finally form and accelerating new innovations everywhere. With these new advancements, our customers can harness that data to build and customize these apps with models faster than ever before while also using Gen AI to make it easier to work with and organize that data. So while we typically think of symbiosis as a relationship between two different things, I like to think of what's happening today as a beneficial relationship among three things, where data, generative AI, and humans are all unleashing their creativity together. Let's explore this relationship for the starting with developing Gen AI apps. To build a Gen AI app, you need a few essentials. You need access to a variety of LLMs and foundational models to meet your needs. You need a secure and private environment to customize these models with your data. Then you need the right tools to build and deploy new applications on top of these models. And you need a really powerful infrastructure to run these data intensive and ML intensive applications. AWS has a long history of providing our customers with the comprehensive set of AI, ML data and compute stack just for this. And now, according to HEG insights, more machine learning workloads run on AWS than any other cloud provider. We like to think of our AI ML offerings as a three layer stack. At the lowest layer is the infrastructure team. You will need to cost effectively train these foundational models and deploy them at scale, including our hardware chips and GPUs. This layer also includes Amazon Sage Baker, our service which enables ML practitioners to easily build, train and deploy ML models, including these LLMs. At the middle layer, we have Amazon Bedrock, which provides access to the leading LLMs and other FM to build and scale-generated AI applications. And at the top layer are the applications that help you take advantage of Chen AI without the need for any specialized knowledge or having to write any code. This includes services like Amazon Cube, our new Chen AI power assistant that is tailored to your business. Each of these layers build on the other and you may need sound or even all of those capabilities at different points in your Chen AI journey. Let's take a closer look at our tools for building Chen AI applications starting with Amazon Bedrock. Bedrock has a broad set of capabilities for building and scaling Chen AI applications with the highest levels of privacy and security. One of the main reasons customers gravitate towards Bedrock is the ability to select from a wide range of leading foundational models that support their unique needs. This customer choice is paramount because we believe no one model will rule the world. We are still in early days with Chen AI and these models will continue to evolve at unprecedented speeds. That's why customers need the flexibility to use different models at different points for different use cases. For this, Bedrock has you covered. We offer customers a broad choice of the latest foundational models from leading providers like AI21 Labs and Tropic, Cohear, Stability and Metal. One of our most popular models is Anthropics Cloud model which is used for tasks like summarization and complex reasoning. Customers also use Stability AI stable diffusion model to generate images, arts and design. Recently, we announced support for Cohear Command which can be used for tasks like copyright and dialogue and Cohear Embed for search and personalization. But, there is more. We recently added support for three new Cohear models, Command Light, Embed English and Embed Multilingual. We also introduced Metas Lama 2 on Bedrock which customers are rapidly adopting for high performance and relatively low cost. And just a couple of weeks ago, we added Lama 2 13B model which is optimized for a variety of smaller in use cases. And finally, we added support for stable diffusion SDxL 1.2. Stability AI's advanced text image model which is now generally available. But then, I told you in this space it's super early and choice is paramount. So today, we are continuing our commitment to offering the latest innovation from many of our model providers starting with Anthropics. I'm excited to announce Bedrock support for Cloud 2.1. Cloud 2.1 delivers advancements in key capabilities for enterprises, including an industry leading 200k context window which also improves your accuracy in many remarkable ways. According to Anthropic, this at date has 50% fewer hallucinations even when you are trying to do adversarial prompt attacks. And it also has a 2x reduction in fall statements in open-ended conversations. Both of these are super important for enterprise use cases. And 2.1 also has improved system prompts which are model instructions that provide a better experience for end users while also reducing the cost of these prompts and completions by 25%. And we are not just stopping there. We are also excited to announce new updates for our customers who want to experiment with publicly available models. That's why today, I'm pleased to announce support for Lama 270B in Bedrock. This model is suitable for many large-scale text-based processing such as language modeling, text generation and dialogue systems. In addition to our partners, AWS is heavily building new innovations in this area for our customers. We have a long-standing history in AI and ML. Amazon has invested in AI ML technologies for over 25 years and many of the capabilities our customers used today are driven by ML models, including these foundational models. These models power virtually everything we do from our customer-facing e-commerce applications to various facets of our enterprise business and supply chain. Because this ML innovation has benefited our business so much, we wanted to share our key learnings with our customers too. One of the ways we want to enable is by enhancing search and personalization experience for our customers was through a specific data type called vector embeddings. Vector embeddings are produced by these foundational models which translate text inputs like words, phrases or large units of text into numerical representations. While VS humans understand text and the meaning and the context of these words, machines only understand numbers. So we have to translate them into a format that is suitable for machine learning. Vectors allow your models to more easily find the relationships between similar words. For instance, a cat is closer to a kitten or a dog is closer to a puppy. This means your foundational models can now produce more relevant responses to your customers. Vectors are ideal for supercharging your application like rich media search and product recommendation. In this scenario, the use of vector embeddings greatly enhances the accuracy of the query like bright color gauges. We have used embeddings to support many aspects of our business like Amazon.com. That's why we offer Titan text embeddings which enables customers like AI company grip tape to easily translate their text data into vector embeddings for a variety of use cases. But as our customers are continuing to build more and more applications, they want to combine image and text that support both modalities. For example, imagine a furniture retail company with thousands of images. They want to enable their customers to search for a furniture using a phrase image or even both. They could use instructions like show me what works well with my sofa. Now to build such kind of experience, developers need to spend time piecing together multiple models. Not only does this increase the complexity of your gen A.I. stack, but it also decreases the efficiency and it impacts customer experience. We wanted to make these applications even easier to build. That's why today I'm excited to announce the general availability of Titan multimodal embeddings. This portal enables you to create richer multimodal search and recommendation options. Now you can quickly generate store and retrieve embeddings to build more accurately and contextually relevant multimodal search. Companies like offer up or using Titan multimodal embeddings as well as Alamed which is using this model to revolutionize their stock image search experience for their customers. In addition to our embedding models, we also offer models that support text generation use cases. This includes Titan text light and Titan text express which are now generally available. These text models help you optimize for accuracy, performance, and cost depending on your use cases. Text light is a really small model, but it is extremely cost-effective model that supports use cases like chatboard, Q&A, and text summarization. It is lightweight and ideal for fine tuning, offering you a highly customizable model for your use case. Text express can be used for a wider range of tasks such as open-ended text generation and conversational chat. This model provides a sweet spot for cost and performance compared to these really big models. Now finally, Chen AI image generation is growing in popularity for industries such as advertising and retail. Where customers need high quality visuals and at a lower cost. We wanted to help our customers do this easily, accurately, and responsibly. That's why today I am very excited to announce Titan image generator which is now available and previewed today. This model enables cost-effective to produce high quality realistic images or enhanced existing images using simple natural language problems. You can customize these images using your own data to create content that better reflects your industry or your brand. Titan Image Generator is trained on diverse set of datasets to enable you to create more accurate outputs. It also includes built-in mitigations for toxicity and bias. Through human evaluation, we found that Titan Image Generator have higher scores than many other leading models. More importantly, to build on our commitments we made at the White House earlier this year to promote the responsible development of AI technology, all Titan generated images come with an invisible watermark designed to help reduce the spread of misinformation by providing a discrete mechanism to identify AI-generated images. AWS is among the first model providers to widely release built-in invisible watermarks that are integrated into image outputs and are designed to be tamper-resistant. Now, let's take a look at the models editing features in action. First, I will use the Image Generator and submit a text prompt such as image of a greenic iguana to get an image quickly to kind of see what I want. Now, I can use the model to easily swap out an existing background. To background offer a rain for us. This process is known as outpainting. You can use the model to seamlessly swap out backgrounds to generate lifestyle images all while retaining the main subject of the image. And to create a few more options, I can use the image playground to generate variations of my original iguana subject as well as variations of the original rain for us background. Or I can completely change the orientation of the picture from left facing to right facing using the prompt like orange iguana facing right in a rain for us. This is really cool, right? And there are so many incredible image-generator features that I cannot cover such as in painting and image customizations that I have not showcased here today. Because this model is trained for a broad range of domains, customers across a variety of industries will be excited to take advantage of Titan Image Generator. As you can see, each Titan model has its own unique strengths across capabilities, price, and performance. And as Adam shared yesterday, we are carefully chosen how we train our models and the data we used to do so. We will identify customers against claims that our models are their output in French on anyone's copyright. With these investments, our customers will have the flexibility to select the best models for their requirements even as their needs grow and change. And our bedrock customers have quickly taken advantage of different models to build all types of customer experiences. In fact, since we launched bedrock, more than 10,000 customers are rapidly developing genie-eyed powered applications for use cases like cell service customer support, text analysis, and forecasting trends. This includes customers like SAP, the world's leading provider of enterprise software, who is using bedrock to automate the creation of trip requests with an SAP concur, saving employees hours of time, including our own Amazon employees. Georgia Pacific, one of the world's leading manufacturers of paper and pulp, used bedrock to power a chat assistant that helps employees quickly retrieve critical factory data and answer questions about their machines. And United Airlines, they use bedrock to help employees access up-to-date information and summaries and delays using natural language, helping them resolve operational issues and customer issues faster. It is so inspiring to see our customers build with these models on bedrock and adapt them for their needs. And now to show you how to put genie-eyed interaction for your business, please welcome Nang Ho, VP of AI from Intude. Good morning, everyone. Great to see you all bright and early today. How you all feeling? Good, come on, that's right. So as I've been listening over the past few days, it's obvious to me why Swami invited us on to this stage. Over the past decade, Intude has built on AWS. First, from moving our application onto the cloud to AIML with SageMaker, and now in the era of generative AI with bedrock. At Intude, everything we do centers around our mission to power prosperity around the world for 100 million consumers and small business customers. And for me, this mission really hits home. I'm one of 10 siblings. Can you imagine being in a family with 10 siblings? There's so many of us, we don't even all fit in the same photo. This is the most of us ever fit in one photo at any one time. And the reason why this hits really close to home for me is that half of my siblings are small business owners. And so I deeply understand that every day challenges that small businesses face for managing inventory to dealing with cash flow, to understanding taxes throughout the year. And so it's really great for me that in my everyday work, I get to make their lives easier with AI. I get to build game changing applications that solve problems for myself, my siblings, and probably a lot of you in the audience here today. At Intude, we're all about leveling the playing field for our customers. And so to do that, we've been on an incredible transformation journey over the past five years. In 2019, we declared that we were going to be an AI-driven expert platform. And by combining cutting edge AI with tax and human expertise, we're delivering unparalleled experiences for our customers. Today, we've been able to achieve incredible scale with AWS running all of our data capabilities as well as our data lake on AWS. And our machine learning platform of which SageMaker is a foundational capability allows our entire community of machine learning developers to build, deploy, and ship new AI experiences with speed. And to give you an idea of what this really means more concretely, this means that we're able to make 65 billion machine learning predictions utilizing over half a million data points for our small business customers, 60,000 for our consumers, and then driving over 810 million customer backed AI interactions per year. Now, in the era of generative AI, we're well positioned to change the game because of our multi-year investment. We've been really honed in on making sure that our data is clean, that there's strong data governance, and that we're building out responsible AI principles. And that's really allowed us to quickly unlock these new opportunities. To enable our technologists to design, build, and quickly ship out AI applications in this Gen AI world, we built a proprietary Gen AI operating system called GenOS, and this is on AWS. And what GenOS has is four primary components. The first is Gen Studio. This is where anybody at the company can build and prototype and test out new to the world Gen AI experiences. When they're ready to ship, then they can use Gen Run Time, which has connectivity to a multitude of LLMs and has access to the right underlying data so that you can build those personalized and accurate experiences, but have the comfort that you can scale it out when needed to customers. The next piece is that when you do deploy and ship these Gen AI experiences, you want that consistency across your products. You don't want a frank and experience. So we built a design system called Gen UX so that you get that transparency. And when a customer interacts with Gen AI, they know that they're getting that experience. And I'd say the most important component to Gen OS is that the series of financial large language models. And this is a set of third-party LLMs, as well as custom-trained LLMs that are specialized in our domain of tax, accounting, marketing, and personal finance. And you may ask, why the heck do I need to do this myself? Why not use what's out there? And I'll tell you a couple things that we learned during this journey is that three things remain constant in the Gen AI world. Accuracy, latency, and cost. Any experience you build will anchor on those three things. And so data is the key to unlocking accuracy. We all know that. But the ability to use these smaller, faster models allows us to realize significant latency gains. And the great thing is that we're able to host these models on SageMaker. And so we're able to finally manage cost as well as scale according to our needs. But I also mentioned earlier, we used third-party LLMs because at the end of the day, the thing that you really optimize for is to build the best customer experience possible. To do that, you need to be able to use best-in-class solutions. And that's what Bedrock gives us the ability to do. It gives you optionality. With the wide library of models available on Bedrock, that Swamy just showed, we're able to fully encompass all of the needs that our customer has. And the other thing that Swamy mentioned is that within Bedrock, we're able to easily scale or underlying inference infrastructure. And all of that is done within our AWS VPC. And so that gives us the confidence to be able to fully leverage our data and our knowledge base to build these personalized, responsible, and also relevant experiences for our customers. But also knowing that the safety, security, and privacy of the data is maintained for our customers. So in September, we launched into a DASIST. A generative AI assistant that is embedded across all of our product offerings. If you go to TurboTops, you go to QuickBooks, you go to MailChimp, you're going to see a SIST. And it's all backed by GenOS. With ASIS, what we're really trying to do is help you feel confident in every single financial decision that you make. It's there with you. And so I'm going to show you what ASIST looks like in TurboTops. This is live in production for our customers. And we also gather significant customer feedback. So if you can imagine when you get to the end of your tax filing experience, you get a number. What does that number mean? I have a PhD in natural physics. I barely know what those numbers mean. And so if you can imagine for the standard everyday user, it's incredibly challenging. By marrying the power of our knowledge engine that ensures accuracy with the power of an LLM, we're able to help unpack this outcome for our customers so that they truly understand and can feel confident to take that next step, whatever it may be. And so for those who are just beginning the AI journeys, we offer two learnings. The first is take a holistic approach. Really invest in your underlying data because that's going to be the differentiator for every single experience that you build. But also build in horizontal solutions from day one. At some point, demos need to become production experiences. And you don't want to get caught by surprise when you're ready to go. The second is that there's no one size fits all LLM solution. Optionality is so incredibly important. And that's what is offered on Bedrock and on AWS. And our collaboration with AWS over the past 10 years has really helped us grow to become a global financial technology platform. And these are just some of the services that have gotten us there. So I agree with Swami, but I have to agree with him on this stage. The massive explosion of data has enabled these foundational models. You saw Assistus one of the experiences that is an outcome of our ability to leverage data and an LLM. At Intuit, Assistus is just going to be one of the many Genai experiences that we build. And over our 40-year history, we've gone through many transformations. Genai is just one of those transformations. And we're going to continue to transform and reinvent over the next 40 years. Thank you so much. Thank you. Thank you now. Intuit is an excellent example of how you can reimagine your customer experience with easy-to-use tools and access to a variety of these foundational models. And as none demonstrated, that is another component that is critical for creating these Genai apps that are unique to your business. That is your data. When you want to build Genai applications that are unique to your business, your data is the differentiator. Data is the key from a generic AI application to a Genai application that understands your customer and your business. So how do you go about customizing these models with your data? A common technique to customizing these foundational models is called fine tuning. With fine tuning, it's pretty simple. You provide a label dataset which are annotated with additional context to train the model on specific tasks. You can then attack the model parameters to your business extending its knowledge with lexicon and terminology that are unique to your industry and your customers. Amazon Bedrock removes the heavy lifting from the fine tuning process. But you can also leverage unlabeled datasets or the raw data to maintain the accuracy of your foundational model for your domain through continued pre-training process. For example, a healthcare company can continue to pre-traine the model using medical journals, articles, or research papers to make it more knowledgeable on the evolving industry terminology. Today, you can leverage both of these techniques with Amazon Titan Light and Titan Express. These models complement each other and will enable your model to understand your business over time. But no matter which method you use, the output model is accessible only to you and it never goes back to the base model. We announced some of these capabilities early on. This week, we also added fine tuning for Bedrock in Bedrock for cohere Command and Lama 2 with fine tuning for Anthropic Claude coming soon. Now, let me just show you a quick example of how this works. Imagine a content marketing manager that needs to come up with a fresh ad campaign copy for a new line of shoes. To do this, they select Lama 2 model and provide Bedrock with a few examples of their best performing campaigns. Bedrock makes a separate copy of the base model that is accessible only to the customer. And after training, Bedrock generates relevant social media content, display ads, and web copy for the new shoes. With fine tuning, you can build applications that are specific to your business. But what if some of your data changes frequently, like inventory you're pricing does? It is simply not practical to be constantly fine tuning and updating this model while it is also serving user queries. That's why, to enable a model with the up-to-date information from your data sources, you need a different technique called retrieval augmenter generation, also known as RAG. With RAG, you can augment the prompt that is sent to your foundational model with contextual information, such as product details, which it draws from your private data sources. These added context in the prompt helps the model provide more accurate and relevant response to the user's query. However, implementing these RAG based systems is extremely complex. Developers must first convert their data into vector embeddings. Then they need to store these embeddings in a vector database that can handle vector queries efficiently. Finally, they build custom integrations with vector database to perform semantic searches, retrieve relevant text, and then augment the prompt. All of this process can take weeks if not months to build. To make this process easier, that's why, yesterday, we announced knowledge bases for Amazon Bedrock, which supports the entire RAG workflow right from ingestion to retrieval to prompt augmentation. Here, you simply point to the location of your data like an s3 bucket and Bedrock fetches relevant context and relevant text documents, converts them into embeddings, and stores it in your vector database. And during inference time, it augments with the right context to your prompts sent to your foundational models. Knowledge bases work with popular vector databases like our vector engine for open-set serverless, Redis Enterprise Cloud, and Pineco. And coming soon, we will also add support for Amazon Aurora as well as MongoDB with more and more databases being added over time. Now, the ability to customize these models with your data is incredibly useful. But you can also extend the power of these models to execute business tasks like booking travel or processing insurance clients. To do this, developers perform several resource intensive steps to fulfill a user request, like defining the instructions and orchestrating or configuring the models to to access your data sources. That's why yesterday, Adam announced the GA of agents for Amazon Bedrock, a capability that enables GEN-AI applications to execute complex tasks by dynamically invoking these APIs. Bedrock makes it super easy to create these fully managed agents that connect to your internal systems and APIs on your behalf in just a few steps. Now that we are talked through different ways to customize your model and remove the heavy lifting with agents, let me walk through a hypothetical scenario on how to leverage GEN-AI capabilities for a task that many of us are familiar with, DIY. How many of you have a home improvement project on your to-do list? I see a few hands. My wife and I did a lot of work in our basement this summer. I believe me. Getting that work done was a full-time job. Any new DIY project requires multiple complex steps. Oftentimes, one of the hardest things of the project is just figuring out how to get started. To help with these challenges, we have built a hypothetical DIY business called Rad DIY. And this is powered by a GEN-AI powered assistant with Clot 2 on Amazon Bedrock. This assistant is designed to remove the complexities of DIY project and provide customers with accurate and easy to follow steps. Let's see how it works. Nina is an ambitious DIYer who wants to replace her bathroom vanity and decides to use the app for her project. She can use natural language to ask the assistant about any type of project and receive a list of detailed steps, materials, and tools along with any necessary permits. The app also leverages customer inputs to generate images of their project using Titan Image Generator model. So after a short interaction, the app provides Nina with a few images for inspiration that she can further refine through conversation and feedback. Once Nina selects the design she likes, our app uses multimodal embeddings to search its extensive inventory and retrieve all of the products she will need. No multiple trips to the store will be necessary. In addition, now, the app uses go here command model to provide a summary of user reviews for each reviewed product. This summarization helps Nina decide if the products and the tools meet her requirements and skill level. Finally, if Nina wants to find a specific item for her vanity like a nautical bronze drawer handles, our app uses the knowledge-based feature in Bedrock to search inventory for products that meet her budget and skill level and time frame. Now that Nina has everything she needs, all that is left for her to do is start her project. I hope this hypothetical scenario sparked some ideas for you on how you can use Bedrock to build Gen A.I. applications with your data to create new customer experiences. We make it easy to get started on Bedrock. But some of our customers also want hands-on support to get started with Gen A.I. That's why we offer AWS Gen A.I Innovation Center, a program that pairs your team with our own expert A.I.M.L scientists and strategy experts to accelerate your Gen A.I. journey. Since we announced it, this program has been getting incredible momentum. Many of our customers also told us they want dedicated support to customizing these foundational models for their needs, which is why we are introducing even more offerings through our Innovation Center. Today, I'm excited to announce a new Innovation Center custom model program for Anthropic Cloud. Applause. Applause. This program, available early next year, will be incredibly powerful because it will enable you to work with our team of experts to customize these highly powerful Cloud models for your business needs with your data. This includes everything from scoping requirements to defining evaluation criteria to working with your proprietary data for fine tuning. You can then securely access and deploy your private models on Bedrock, which will be available only in your VPC. However, customizing these foundational models isn't the only way to build innovative AI applications. As I mentioned earlier, that we still be need for certain companies to build their own. And these customers need powerful machine learning infrastructure. For instance, AWS has partnered with NVIDIA for 13 years to deliver large-scale, high-performance GPU solutions that are widely used for deep learning workloads. And this week, we announced an expansion of a strategic collaboration to deliver next generation infrastructure, software, and services for a generative AI. And to provide more choice for our customers, we have invested in our own ML chips, AWS Training and AWS Inferential, to push the boundaries on cost efficiency and performance. We also enable our customers with best-in-class software tools in the software layer of the stack with Amazon SageMaker. SageMaker makes it easy for customers to build, train, and deploy ML models, including these LLMs, with tools and workflows for the entire ML lifecycle, right from data preparation to model deployment. We have also invested in providing efficient model training with distributed training libraries and built-in tools to improve model performance. And today, leading organizations like Stability AI, AI21 Labs, Hugging Face, and TII are training their foundational models on Amazon SageMaker. But with all of these investments in this area, training a foundational model can still be incredibly challenging. First, customers need to acquire large amounts of data, create and maintain a large cluster of accelerators, right code to distribute model training across a cluster, frequently inspect and optimize the model, and manually remediate any hardware issues. And all of these steps require deep ML expertise. Let me dive into some of these challenges to understand why it is so complex. Because of the massive size of these foundational models, and the data set used for training, developers need to split their data into chunks and load them into the individual chips in a training cluster, a distributed cluster, with hundreds or even thousands of accelerators. This is a lot of work, because in order to make efficient use of these compute and network resources, the distribution needs to be tailored to the characteristics of the data. Your model architecture, as well as the underlying hardware configurations. That means you have to write a lot of code and optimize it frequently. In addition, customers need to frequently pause and inspect the model performance, optimize the code if something is not working right. To do this, they had to manually take checkpoints of the model state so that the training is able to start without any loss in progress. Finally, when any of these thousands of accelerators in the cluster fail, the entire training process is halted. To resolve this issue, customers had to identify, isolate, repair, and recover the faulty instance, or change the configuration of the entire cluster further delaying the progress. We wanted to make it easier for our customers to train these LLMs without interruption or delays. That's why today I am thrilled to announce the general availability of SageMaker hyperpods. APPLAUSE This one is a big deal because it's a new distributed training capability that can reduce model training time by up to 40%. Hyperpods is pre-configured with SageMaker's distributed training libraries. This enables your data and models to efficiently distribute across thousands of chips in the cluster and process them in parallel. The hyperpods helps customers iteratively pause, inspect, and optimize these models because it automatically takes checkpoints frequently. And if a hardware failure occurs, it detects the failure, it replaces the faulty instance, and resumes the training from the last eight checkpoint. With this new capability, customers will see dramatic improvements by training models for weeks, if not months, without any disruption. But this is just one of the many innovations we announced for SageMaker this week. Today, we are announcing the slew of new SageMaker features across inference, training, and MLOBS. APPLAUSE I had to extend by an R if I had to cover all of it, but I'll just do a quick hit and say, SageMaker inference reduces model deployment by 50% on average and achieves better latency by 20%. We also introduced new capabilities in SageMaker Studio like a new user experience. And all of these updates help customers build, train, and deploy these new large language models even easier. I encourage you to check out Bratinsahas innovation session later today to learn more about these innovations. And now, I'd like to introduce one of the customers who is leveraging some of these latest SageMaker innovations and training their own, training and deploying their own models. Please join me in welcoming Arvind Shrinivas, CEO, and co-founder of Public City to the stage. MUSIC At Public City, we strive to be the world's leading conversational answer engine that directly answers your questions, which references provided to you in the form of citations. Our company is reimagining the future of search by trying to take us from 10 blue links to personalize answers that cut through the noise and gets to exactly what you want. Proplexity's co-pilot is an interactive search companion. As you see, it starts with a general question that you have in your mind, digs deeper to clarify your needs, and after a few interactions with you, gives you a great answer. Ars is the first global publicly deployed example of generative user interfaces that reduces the need for prompt engineering. This is such a complex product to run and a hard problem to solve. Hence why we decided to go all in on AWS. We started off by testing frontier models like Anthropics Cloud 2 on AWS Bedrock. Bedrock provides cutting edge inference for these frontier models. This helped us to quickly test and deploy Cloud 2 to improve our general question answering capabilities by providing more natural sounding answers. Cloud 2 has also had to inject new capabilities into Proplexity's product, like the ability to upload multiple large files and ask questions about their contents, helping us to be the leading research assistant there is in the market. But Proplexity is not just the wrapper on top of close proprietary large language model APIs. Instead, we orchestrate several different models in one single product, including those that we have trained ourselves. We built on top of open source models like Lama 2 and Mistro and fine tuned them to be accurate and live with no knowledge cut off by grounding them with web search data using cutting edge rack. This is when we started working with the AWS Startup team on an Amazon SageMaker Hyperbot\n",
      "[SOURCEFILE:  Building the more intelligent future of cloud Clay Magouyrk keynote  Oracle CloudWorld 2023.txt]now have an environment that's probably a lot more maintainable. Yeah. Exactly. So I just freed up smart engineers to work on other problems than maintaining this code. Some of the other benefits that we ran into, we noticed a 50% increase in performance of our queries. And this is both for the analytics queries as well as our normal transactional work. That's great. That's great. So I understand you're also using heatwave for machine learning now. Is that right? Yeah. So a lot of the people, my colleagues at Nvidia are doing really exciting things with AI. Self-driving cars or medical advances. I wanted to look at how can I help have AI help me and my team with our operational excellence. And so we started a proof of concept using heatwaves, auto-ML, log anomaly detection. And we coordinated with Oracle's team and we're drawing the OCI's native logs, collaborating that with the database error logs. And the idea is to find the early detection of errors before they actually happen. So if we can, for example, identify that a file system is running low. That's a yellow flag situation. But we can generate a ticket to an operator who can then either grow the system or remove files and prevent the outers that would happen if we hadn't addressed it. So obviously this means it's more for those that rely on our services. But the added benefit is the operational support effort for preventing that outage is much less than it would be recovering from an outage. So now we're actually starting to coordinate what we're getting out of this analysis with our monitoring systems so that we can measure a reduction in outage time. So just one follow-up question, Matt, did you have to hire any data scientists to do this machine learning project? No, this is the greatest part about it because all of this was built into the heatwave service that we're already using. So we didn't have any more costs. There's no additional overhead. We didn't have to build an application for this. It was so easy to use and we're storing our data in a native format. So it greatly reduced the complexity of the solution and it helped us to get to production much more quickly. That's great. I'm really glad to hear that. So if you'd like to learn more about my SQL heatwave and Lake House and AutoML, you can join me today at 4 o'clock in Ballroom E. We're giving a keynote about it. And with that, I'd like to really thank you Chris for coming and talking to you today. Thank you so much. Thank you. Thank you. All right. Thank you, Edward. It's great to hear from Edward and Chris about the success they're having with my SQL heatwave. Now I know that we haven't talked enough about AI, but we're now getting to that section. I've shared a lot about alloy, our partnerships with Microsoft. You've heard Edward talk about heatwave. The thing to understand is that Oracle is really committed to delivering a distributed cloud that delivers our differentiated services wherever you need them. So we started this conversation by asking our generative AI service powered by CoHear, what is the Oracle Cloud? And given the recent advancements, it would obviously be wrong for me not to spend some time talking about generative AI. So to do that, let's start by talking about what generative AI is. Fundamentally, it predicts what comes next based on what's come before. And this is a correct interpretation, but it's also overly simplistic. Amazing things are built out of very simple primitives. Like for example, a general purpose computer. You have a simple architecture, you have a very simple instruction set, and through a massive combination of all those different instructions, you get the technology that we've built as a species over the past 40, 50 years. So saying that generative AI is not reasoning or it's not doing what a human would do, those things are true, but there are also misses the point. So let's take a look at some examples of what generative AI can do rather than just talking about it theoretically. Here's an example where a healthcare provider is using a simple prompt to generate a letter it needs to send to an insurance company that authorizes a brain imaging procedure. And this is done entirely based on doctor's notes. So this isn't a task that would normally be done by a person. It's completely automated based on existing data. If you're like me, you're in many meetings every day. Sometimes you miss one and all you get is a transcript or a recording of those meetings and it's a lot of content to parse through. These models can extract and summarize the relevant information. In this example, a simple prompt extracts and summarizes the action items along with the people that they're assigned to. These models can also transform. Here's an example of a chat between two colleagues who were reviewing a proposal for a customer. In this case, we've integrated the model so that it detects harsh language and suggests a more professional wording before this is sent to your colleague. These models can also do things that humans traditionally understand as reasoning. How many times have you looked at an insurance policy? And you have a question about some of the details covered in this very long document. In this case, you can ask the model a question and it will return the answer along with references to the document, explaining exactly where that information lives. Machine learning has been with us for many years. It's become ubiquitous in many ways. We don't even realize when we're using it. Whether it be or uploading a picture through social media and it uses facial recognition to help us tag our friends, whether our spam filter on our email system reduces the amount of clutter that gets sent our way, all of those are great examples of machine learning. So if machine learning has been around for years, what is it about now that makes generative AI happen? Well, it's a combination of multiple factors. First, it starts with better software. The transformer model, which came out of the paper from Google in 2017, is about neural networks that learn context and thus meaning. This was focused on taking large sets of data and understanding the relationships between that data. Stanford researchers called these Transformers Foundation models and this has been a paradigm shift in the AI space. At the same time this is happening and over the next few years, we have massive advancements in the hardware to support the training of these models. Whether it be through general purpose GPUs or dedicated ASICs, we have more than 100 fold increase in the compute power we can use to train these AI models. And at the same time that you're seeing the individual compute power of a single unit increase, we also have higher bandwidth networks, lower latency networks, and we have technology that enables you to cluster together more of those GPUs than has ever been possible before. So at OCI, we call that combination of hardware and networking our super clusters. Today we support more than 16,000 GPUs that can each talk to each other from any GPU to any other GPU in less than 20 microseconds, all with 400 gigabits per second of throughput for each GPU to every other GPU. Customers like Recha and Mosaic and N-World use these super clusters today to train their massive models. And Mosaic as an example reports 50% better performance and 80% cost savings by running the Sun OCI. So as an industry, we've made substantial investments into the hardware and networking to support these workloads. But there are other barriers to adopting this technology. First, when I talk to customers, their biggest barrier is getting access to it. And when they do have access, how do they control the technology? How do they make sure they know where their data is going and how it's being used? So to solve that problem, we started with the best models from our partner co here. We then worked together to deliver an enterprise class, customized, fully controlled service, which is our new generative AI service. As of today, we're announcing that's an early access and it will be in production very soon. The key that we're focusing on with this service is ensuring that customers retain complete control of their data and understand exactly how it's being used. That service will be available everywhere in our distributed cloud model. Once you get past the access and control barrier, the next barrier I experience when I talk to customers is really, it's a lack of experience. This is not unique to AI. The reality is, anytime we have a new technology trend, employees or people, they just don't have experience with it so far. So what do you do? The thing is, you've got to play around with it, right? We've all been children at one time or the other. We're more excited about what we can learn rather than worrying about what we're going to lose. So the way to solve this problem is to make sure that you take access to the technology, give it to your teams in a controlled and safe manner. And then you'll end up being extremely surprised by the things that they create with it that you couldn't even imagine before. So once you have that experience and you've got controlled access, what I see as a next barrier is how do I integrate that technology into my existing applications and workloads? To solve this problem, what we've done is we've worked in conjunction with CoHear to create a great demonstration of how these foundational models combined with retrieval augmented generation can actually make it very easy for you to integrate this into your workloads. So let's take a look at that. So, how does that work? How does an application powered by generative AI get you to the answers you need faster? Natalie, our customer, is interacting with an application powered by a large language model. And this application does more than just chat. It's connected to a number of back end systems like the user, knowledge base and inventory systems. Natalie is trying to figure out why her generator isn't starting up. She can ask her question however she likes, and the system responds naturally, and also includes links to more information. The application is using a large language model to understand Natalie's request and search a collection of information for the answer. In this case, the system is looking in a knowledge base and will include links to the relevant items with a response. The system has provided some possible solutions, and Natalie has decided it must be an issue with the Spark plug. She can ask a follow-up question like which Spark plug she needs. The application is going back to the knowledge base, but it does it while maintaining the context of the conversation. She doesn't have to restate anything like her generator model. Now that Natalie knows the right part, she wants to know where to get it. The system finds the part in nearby locations, and also finds online options. The application can turn to other systems like a customer database to first discover where Natalie is located, and then an inventory system to look for matching parts. A large language model can translate the natural language prompt into an SQL query to get the information from these systems. Being able to look across a full data landscape allows the application to return richer responses. Natalie opts for an online option and the system orders the part. With permission to get more information from the customer database and the ability to place an order, the application can perform an action like creating an online order for Natalie. Applications powered by Generative AI get you to your answers faster by allowing you to interact naturally, binding the information you need across systems and supporting the actions needed to get the job done. Work smarter with OCI Generative AI. I don't know about you. I look at demonstrations like that and I think it brings together all of these individual pieces that we're thinking about. In addition, one of the favorite integrations I have are the ones that I don't have to do. As Oracle, because we're also an applications company, I want to show you briefly how we can actually pre-integrate this great technology in the existing applications. Our HCM team has integrated AI-assisted goal set. Our Net Suite team has integrated AI-driven job description generation. Across all of our industry verticals, we are using Generative AI in industry-specific ways. Here, the construction and engineering team is built a scheduling AI assistant to generate complete project schedules for you. When you take all of this and combine it together, the end result is a complete cloud AI solution that gives you everything you need from an infrastructure layer to train and inference these models across the best models and services available in your entire cloud portfolio and these things are pre-embedded in your applications. Another important special property of the Oracle Cloud is that we're continuously learning. Today, we've covered many of the special properties that Oracle is about delivering a unified cloud that combines the best of applications in infrastructure, those workloads are delivered sustainably, wherever customers need them met, and that the entire portfolio is continuously learning and improving. What I want you to walk away with today is that the pace of innovation and technology is accelerating across the board. You have a choice. You can either be afraid of this high rate of change or you can instead choose to embrace it. I encourage you to experiment. Go out and play with new technology. Obviously, that's true for Generative AI, but it's true across everything that we've talked about today. As that pace of change increases, you're going to find if you embrace it that there are many opportunities you didn't imagine before, and that instead of being disrupted, you can really surprise yourself and your customers with the great things that you can build. Throughout that journey, the Oracle Cloud is here for you. Thank you very much.\n"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search\n",
    "query = \"satya keynote nvidia\"  \n",
    "  \n",
    "results = search_client.search_hybrid(query)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid\n",
    "query = \"bedrock\"  \n",
    "  \n",
    "search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "  \n",
    "vector_query = VectorizedQuery(vector=get_embedding(query), k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "  \n",
    "category = \"AWS\"\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    vector_filter_mode=VectorFilterMode.PRE_FILTER,\n",
    "    filter=f\"category eq '{category}'\",\n",
    "    #filter=\"category eq 'Microsoft'\",\n",
    "    select=[\"title\", \"content\", \"category\", \"spr\"],\n",
    ")\n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['content']}\")  \n",
    "    print(f\"Category: {result['category']}\\n\")  \n",
    "    print(f\"SPR: {result['spr']}\\n\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
